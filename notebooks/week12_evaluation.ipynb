{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2e329e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# notebooks/week12_evaluation.ipynb\n",
    "\n",
    "\"\"\"\n",
    "# Week 12: Evaluating & Hardening LLM Apps\n",
    "## Production-Ready Healthcare Assistant\n",
    "\n",
    "### Learning Objectives\n",
    "1. Implement comprehensive RAG evaluation\n",
    "2. Detect and prevent hallucinations\n",
    "3. Run safety red-team tests\n",
    "4. Set up regression testing\n",
    "5. Add production monitoring\n",
    "6. Complete the capstone project\n",
    "\n",
    "### Setup\n",
    "\"\"\"\n",
    "\n",
    "# Cell 1: Setup\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "project_root = Path.cwd().parent\n",
    "sys.path.insert(0, str(project_root))\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "print(\"OpenAI API Key:\", \"‚úÖ\" if os.getenv(\"OPENAI_API_KEY\") else \"‚ùå\")\n",
    "\n",
    "\n",
    "# Cell 2: Load RAG System\n",
    "\"\"\"\n",
    "## Part 1: Setup RAG for Evaluation\n",
    "\"\"\"\n",
    "\n",
    "from src.llm.rag import VectorStoreManager, get_vector_store\n",
    "from src.llm.rag.chains import RAGChain\n",
    "\n",
    "# Load existing vector store\n",
    "vector_store = get_vector_store()\n",
    "\n",
    "try:\n",
    "    vector_store.load(\"healthcare_policies\")\n",
    "    print(\"‚úÖ Vector store loaded\")\n",
    "except:\n",
    "    print(\"‚ö†Ô∏è Vector store not found - creating new one\")\n",
    "    from src.llm.rag import load_policy_documents\n",
    "    docs = load_policy_documents(\"data/documents\")\n",
    "    vector_store.create_from_documents(docs)\n",
    "    vector_store.save(\"healthcare_policies\")\n",
    "\n",
    "# Create RAG chain\n",
    "rag_chain = RAGChain(vector_store)\n",
    "print(\"‚úÖ RAG chain ready\")\n",
    "\n",
    "\n",
    "# Cell 3: RAG Quality Evaluation with Ragas\n",
    "\"\"\"\n",
    "## Part 2: RAG Quality Metrics\n",
    "\"\"\"\n",
    "\n",
    "from src.llm.evaluation import RagasEvaluator\n",
    "from src.llm.rag.evaluation import create_healthcare_golden_set\n",
    "\n",
    "# Create golden set\n",
    "golden = create_healthcare_golden_set()\n",
    "print(f\"Golden set has {len(golden.questions)} questions\")\n",
    "\n",
    "# Create evaluator\n",
    "evaluator = RagasEvaluator(\n",
    "    thresholds={\n",
    "        \"faithfulness\": 0.7,\n",
    "        \"answer_relevancy\": 0.7,\n",
    "        \"context_precision\": 0.6\n",
    "    }\n",
    ")\n",
    "\n",
    "# Get questions\n",
    "questions, ground_truths = golden.to_eval_format()\n",
    "\n",
    "# Run on subset for demo\n",
    "print(\"\\nEvaluating RAG quality...\")\n",
    "evaluator.add_samples_from_chain(rag_chain, questions[:5], ground_truths[:5])\n",
    "results = evaluator.evaluate()\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"RAG QUALITY RESULTS\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Evaluator: {results['evaluator']}\")\n",
    "print(f\"Pass Rate: {results['pass_rate']:.1%}\")\n",
    "print(\"\\nMetric Summary:\")\n",
    "for metric, values in results.get('summary', {}).items():\n",
    "    print(f\"  {metric}: {values['mean']:.3f} (min: {values['min']:.3f}, max: {values['max']:.3f})\")\n",
    "\n",
    "\n",
    "# Cell 4: Hallucination Detection\n",
    "\"\"\"\n",
    "## Part 3: Hallucination Detection\n",
    "\"\"\"\n",
    "\n",
    "from src.llm.evaluation import HallucinationDetector\n",
    "\n",
    "detector = HallucinationDetector()\n",
    "\n",
    "# Test with a sample\n",
    "test_question = \"What are the consequences of missing appointments?\"\n",
    "result = rag_chain.ask(test_question, return_sources=True)\n",
    "\n",
    "detection = detector.detect(\n",
    "    question=test_question,\n",
    "    answer=result[\"answer\"],\n",
    "    contexts=[s[\"content\"] for s in result.get(\"sources\", [])]\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"HALLUCINATION DETECTION\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Question: {test_question}\")\n",
    "print(f\"\\nAnswer: {result['answer'][:300]}...\")\n",
    "print(f\"\\nHas Hallucination: {detection.has_hallucination}\")\n",
    "print(f\"Confidence: {detection.confidence:.2f}\")\n",
    "print(f\"Claims Checked: {detection.claims_checked}\")\n",
    "print(f\"Claims Verified: {detection.claims_verified}\")\n",
    "\n",
    "if detection.issues:\n",
    "    print(\"\\nIssues Found:\")\n",
    "    for issue in detection.issues:\n",
    "        print(f\"  - {issue['claim'][:50]}...\")\n",
    "        print(f\"    Status: {issue['status']}\")\n",
    "\n",
    "\n",
    "# Cell 5: Batch Hallucination Check\n",
    "\"\"\"\n",
    "### Batch Hallucination Check\n",
    "\"\"\"\n",
    "\n",
    "# Test multiple responses\n",
    "test_questions = [\n",
    "    \"What is the cancellation policy?\",\n",
    "    \"When are reminders sent?\",\n",
    "    \"What happens after 3 no-shows?\",\n",
    "    \"Can fees be waived?\"\n",
    "]\n",
    "\n",
    "hallucination_results = []\n",
    "\n",
    "for question in test_questions:\n",
    "    result = rag_chain.ask(question, return_sources=True)\n",
    "    detection = detector.detect(\n",
    "        question=question,\n",
    "        answer=result[\"answer\"],\n",
    "        contexts=[s[\"content\"] for s in result.get(\"sources\", [])]\n",
    "    )\n",
    "    hallucination_results.append(detection)\n",
    "\n",
    "# Summary\n",
    "summary = detector.get_summary(hallucination_results)\n",
    "print(\"\\nBatch Hallucination Summary:\")\n",
    "print(f\"  Total Samples: {summary['total_samples']}\")\n",
    "print(f\"  With Hallucination: {summary['samples_with_hallucination']}\")\n",
    "print(f\"  Hallucination Rate: {summary['hallucination_rate']:.1%}\")\n",
    "print(f\"  Claim Support Rate: {summary['claim_support_rate']:.1%}\")\n",
    "\n",
    "\n",
    "# Cell 6: Safety Testing\n",
    "\"\"\"\n",
    "## Part 4: Safety Red-Team Testing\n",
    "\"\"\"\n",
    "\n",
    "from src.llm.evaluation import SafetyEvaluator\n",
    "\n",
    "safety_eval = SafetyEvaluator()\n",
    "safety_eval.load_default_tests()\n",
    "\n",
    "print(f\"Loaded {len(safety_eval.tests)} safety tests\")\n",
    "print(\"\\nTest Categories:\")\n",
    "categories = set(t.category.value for t in safety_eval.tests)\n",
    "for cat in categories:\n",
    "    count = sum(1 for t in safety_eval.tests if t.category.value == cat)\n",
    "    print(f\"  - {cat}: {count} tests\")\n",
    "\n",
    "\n",
    "# Cell 7: Run Safety Tests\n",
    "\"\"\"\n",
    "### Run Safety Tests\n",
    "\"\"\"\n",
    "\n",
    "print(\"\\nRunning safety tests...\")\n",
    "safety_results = safety_eval.run_tests(rag_chain)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"SAFETY TEST RESULTS\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Total Tests: {safety_results['total_tests']}\")\n",
    "print(f\"Passed: {safety_results['passed']}\")\n",
    "print(f\"Failed: {safety_results['failed']}\")\n",
    "print(f\"Pass Rate: {safety_results['pass_rate']:.1%}\")\n",
    "\n",
    "print(\"\\nBy Category:\")\n",
    "for cat, data in safety_results['by_category'].items():\n",
    "    status = \"‚úÖ\" if data['failed'] == 0 else \"‚ùå\"\n",
    "    print(f\"  {status} {cat}: {data['passed']}/{data['passed'] + data['failed']} passed\")\n",
    "\n",
    "if safety_results['failed_tests']:\n",
    "    print(\"\\nFailed Tests:\")\n",
    "    for test in safety_results['failed_tests'][:5]:\n",
    "        print(f\"  ‚ùå {test['name']} ({test['severity']})\")\n",
    "        for reason in test['reasons'][:2]:\n",
    "            print(f\"     - {reason}\")\n",
    "\n",
    "\n",
    "# Cell 8: Get Security Recommendations\n",
    "\"\"\"\n",
    "### Security Recommendations\n",
    "\"\"\"\n",
    "\n",
    "recommendations = safety_eval.get_recommendations()\n",
    "\n",
    "print(\"\\nSecurity Recommendations:\")\n",
    "for i, rec in enumerate(recommendations, 1):\n",
    "    print(f\"\\n{i}. {rec}\")\n",
    "\n",
    "\n",
    "# Cell 9: Custom Metrics\n",
    "\"\"\"\n",
    "## Part 5: Custom Metrics\n",
    "\"\"\"\n",
    "\n",
    "from src.llm.evaluation import EvaluationMetrics\n",
    "\n",
    "metrics = EvaluationMetrics()\n",
    "\n",
    "# Test response\n",
    "question = \"How do I reschedule my appointment?\"\n",
    "result = rag_chain.ask(question)\n",
    "answer = result[\"answer\"]\n",
    "\n",
    "# Calculate all metrics\n",
    "all_metrics = metrics.calculate_all(\n",
    "    question=question,\n",
    "    answer=answer,\n",
    "    contexts=[]\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"CUSTOM METRICS\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Question: {question}\")\n",
    "print(f\"Answer: {answer[:200]}...\")\n",
    "\n",
    "print(\"\\nMetric Results:\")\n",
    "for name, result in all_metrics.items():\n",
    "    status = \"‚úÖ\" if result.passed else \"‚ùå\"\n",
    "    print(f\"  {status} {name}: {result.score:.3f} (threshold: {result.threshold})\")\n",
    "\n",
    "summary = metrics.get_summary(all_metrics)\n",
    "print(f\"\\nOverall Pass Rate: {summary['overall_pass_rate']:.1%}\")\n",
    "\n",
    "\n",
    "# Cell 10: Regression Testing\n",
    "\"\"\"\n",
    "## Part 6: Regression Testing\n",
    "\"\"\"\n",
    "\n",
    "from src.llm.evaluation import RegressionTestSuite\n",
    "\n",
    "# Create a baseline\n",
    "baseline_questions = [\n",
    "    \"What is the no-show policy?\",\n",
    "    \"How many reminders do patients receive?\",\n",
    "    \"What are the consequences of missing appointments?\",\n",
    "    \"Can appointment fees be waived?\",\n",
    "    \"How do I cancel an appointment?\"\n",
    "]\n",
    "\n",
    "suite = RegressionTestSuite(tolerance=0.05)\n",
    "\n",
    "# Create baseline from current system\n",
    "print(\"Creating baseline...\")\n",
    "baseline = suite.create_baseline(rag_chain, baseline_questions)\n",
    "\n",
    "print(f\"Baseline created with {len(baseline['tests'])} tests\")\n",
    "\n",
    "# Save baseline\n",
    "baseline_path = project_root / \"evals\" / \"baselines\" / \"demo_baseline.json\"\n",
    "suite.save_baseline(baseline, str(baseline_path))\n",
    "print(f\"Saved to: {baseline_path}\")\n",
    "\n",
    "\n",
    "# Cell 11: Run Regression Tests\n",
    "\"\"\"\n",
    "### Run Regression Tests\n",
    "\n",
    "Simulating testing against baseline (should pass since it's the same system)\n",
    "\"\"\"\n",
    "\n",
    "# Load and run\n",
    "suite2 = RegressionTestSuite()\n",
    "suite2.load_baseline(str(baseline_path))\n",
    "\n",
    "print(\"Running regression tests...\")\n",
    "regression_results = suite2.run(rag_chain)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"REGRESSION TEST RESULTS\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Passed: {regression_results['passed']}\")\n",
    "print(f\"Tests: {regression_results['tests_passed']}/{regression_results['total_tests']}\")\n",
    "print(f\"Average Similarity: {regression_results['similarity_score']:.2f}\")\n",
    "\n",
    "if regression_results['failed_tests']:\n",
    "    print(\"\\nFailed Tests:\")\n",
    "    for test in regression_results['failed_tests']:\n",
    "        print(f\"  ‚ùå {test['question'][:40]}...\")\n",
    "        print(f\"     Similarity: {test['similarity']:.2f}\")\n",
    "\n",
    "\n",
    "# Cell 12: Full Evaluation Framework\n",
    "\"\"\"\n",
    "## Part 7: Complete Evaluation Framework\n",
    "\"\"\"\n",
    "\n",
    "from src.llm.evaluation import EvaluationFramework, EvaluationConfig\n",
    "from src.llm.evaluation.framework import EvaluationType\n",
    "\n",
    "# Configure evaluation\n",
    "config = EvaluationConfig(\n",
    "    evaluation_types=[\n",
    "        EvaluationType.RAG_QUALITY,\n",
    "        EvaluationType.HALLUCINATION,\n",
    "        EvaluationType.SAFETY,\n",
    "        EvaluationType.PERFORMANCE\n",
    "    ],\n",
    "    rag_thresholds={\n",
    "        \"faithfulness\": 0.6,\n",
    "        \"answer_relevancy\": 0.6,\n",
    "        \"context_precision\": 0.5\n",
    "    },\n",
    "    hallucination_threshold=0.3,\n",
    "    safety_pass_rate=0.8,\n",
    "    max_latency_ms=10000\n",
    ")\n",
    "\n",
    "# Create framework\n",
    "framework = EvaluationFramework(config)\n",
    "framework.register_rag_chain(rag_chain)\n",
    "framework._golden_set = golden\n",
    "\n",
    "print(\"Running full evaluation suite...\")\n",
    "report = framework.run_full_evaluation()\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"FULL EVALUATION REPORT\")\n",
    "print(\"=\" * 60)\n",
    "print(report.to_markdown()[:2000])\n",
    "\n",
    "\n",
    "# Cell 13: Production Monitoring\n",
    "\"\"\"\n",
    "## Part 8: Production Monitoring\n",
    "\"\"\"\n",
    "\n",
    "from src.llm.production.monitoring import LLMMonitor, MetricsCollector\n",
    "\n",
    "# Create monitor\n",
    "monitor = LLMMonitor()\n",
    "\n",
    "# Simulate some requests\n",
    "import random\n",
    "import time\n",
    "\n",
    "print(\"Simulating production traffic...\")\n",
    "\n",
    "for i in range(20):\n",
    "    with monitor.track_request(\"gpt-4o-mini\") as tracker:\n",
    "        # Simulate request\n",
    "        time.sleep(random.uniform(0.1, 0.5))\n",
    "        tracker.set_tokens(random.randint(100, 500))\n",
    "    \n",
    "    # Simulate cache\n",
    "    monitor.record_cache(hit=random.random() > 0.5)\n",
    "\n",
    "# Get stats\n",
    "stats = monitor.get_stats(window_minutes=5)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"PRODUCTION METRICS\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Requests: {stats['requests']['total']}\")\n",
    "print(f\"Avg Latency: {stats['latency_ms']['avg']:.0f}ms\")\n",
    "print(f\"P95 Latency: {stats['latency_ms']['p95']:.0f}ms\")\n",
    "print(f\"Error Rate: {stats['errors']['rate']:.1%}\")\n",
    "print(f\"Cache Hit Rate: {stats['cache']['hit_rate']:.1%}\")\n",
    "\n",
    "# Get health\n",
    "health = monitor.get_health()\n",
    "print(f\"\\nHealth Status: {health['status']}\")\n",
    "\n",
    "\n",
    "# Cell 14: Error Handling Demo\n",
    "\"\"\"\n",
    "## Part 9: Error Handling\n",
    "\"\"\"\n",
    "\n",
    "from src.llm.production.error_handling import (\n",
    "    safe_llm_call,\n",
    "    CircuitBreaker,\n",
    "    ErrorHandler\n",
    ")\n",
    "\n",
    "# Demo safe_llm_call decorator\n",
    "@safe_llm_call(max_retries=3, fallback=\"I apologize, I'm having trouble processing that.\")\n",
    "def generate_response(question):\n",
    "    return rag_chain.ask(question)[\"answer\"]\n",
    "\n",
    "# Test it\n",
    "response = generate_response(\"What is the cancellation policy?\")\n",
    "print(f\"Response: {response[:200]}...\")\n",
    "\n",
    "\n",
    "# Cell 15: Circuit Breaker\n",
    "\"\"\"\n",
    "### Circuit Breaker Pattern\n",
    "\"\"\"\n",
    "\n",
    "# Demo circuit breaker\n",
    "breaker = CircuitBreaker(\n",
    "    failure_threshold=3,\n",
    "    recovery_timeout=30,\n",
    "    success_threshold=2\n",
    ")\n",
    "\n",
    "print(f\"Circuit Breaker State: {breaker.state}\")\n",
    "\n",
    "# Simulate usage\n",
    "@breaker\n",
    "def protected_call(question):\n",
    "    return rag_chain.ask(question)[\"answer\"]\n",
    "\n",
    "try:\n",
    "    result = protected_call(\"Test question\")\n",
    "    print(f\"‚úÖ Call succeeded, state: {breaker.state}\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Call failed: {e}\")\n",
    "\n",
    "\n",
    "# Cell 16: Capstone - Complete System Test\n",
    "\"\"\"\n",
    "## Part 10: Capstone - Complete Healthcare Assistant\n",
    "\n",
    "Putting it all together: A production-ready healthcare assistant\n",
    "with evaluation, safety, and monitoring.\n",
    "\"\"\"\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"CAPSTONE: HEALTHCARE APPOINTMENT ASSISTANT\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# 1. System Check\n",
    "print(\"\\nüìã System Check:\")\n",
    "print(f\"  ‚úÖ RAG Chain: Ready\")\n",
    "print(f\"  ‚úÖ Vector Store: {vector_store.get_stats()['metadata'].get('chunk_count', 0)} chunks\")\n",
    "\n",
    "# 2. Sample Interaction\n",
    "print(\"\\nüí¨ Sample Interaction:\")\n",
    "test_q = \"What happens if I miss my appointment?\"\n",
    "response = rag_chain.ask(test_q, return_sources=True)\n",
    "print(f\"  Q: {test_q}\")\n",
    "print(f\"  A: {response['answer'][:200]}...\")\n",
    "\n",
    "# 3. Quality Check\n",
    "print(\"\\nüìä Quality Metrics:\")\n",
    "metrics_result = metrics.calculate_all(test_q, response['answer'])\n",
    "passed = sum(1 for r in metrics_result.values() if r.passed)\n",
    "print(f\"  Metrics Passed: {passed}/{len(metrics_result)}\")\n",
    "\n",
    "# 4. Hallucination Check\n",
    "print(\"\\nüîç Hallucination Check:\")\n",
    "h_result = detector.detect(\n",
    "    test_q, \n",
    "    response['answer'],\n",
    "    [s['content'] for s in response.get('sources', [])]\n",
    ")\n",
    "print(f\"  Clean: {'‚úÖ Yes' if not h_result.has_hallucination else '‚ùå No'}\")\n",
    "\n",
    "# 5. Safety Status\n",
    "print(\"\\nüõ°Ô∏è Safety Status:\")\n",
    "print(f\"  Tests Passed: {safety_results['passed']}/{safety_results['total_tests']}\")\n",
    "\n",
    "# 6. System Health\n",
    "print(\"\\n‚ù§Ô∏è System Health:\")\n",
    "print(f\"  Status: {health['status'].upper()}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"CAPSTONE COMPLETE! üéâ\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "\n",
    "# Cell 17: Save Final Report\n",
    "\"\"\"\n",
    "## Save Final Evaluation Report\n",
    "\"\"\"\n",
    "\n",
    "# Save comprehensive report\n",
    "report_dir = project_root / \"evals\" / \"final_report\"\n",
    "report_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Save evaluation report\n",
    "report.save(str(report_dir / \"evaluation_report.json\"))\n",
    "\n",
    "# Save safety results\n",
    "import json\n",
    "with open(report_dir / \"safety_results.json\", 'w') as f:\n",
    "    json.dump(safety_results, f, indent=2)\n",
    "\n",
    "# Save regression baseline\n",
    "suite.save_baseline(baseline, str(report_dir / \"regression_baseline.json\"))\n",
    "\n",
    "print(f\"Reports saved to: {report_dir}\")\n",
    "print(\"\\nFiles created:\")\n",
    "for f in report_dir.iterdir():\n",
    "    print(f\"  - {f.name}\")\n",
    "\n",
    "\n",
    "# Cell 18: Course Summary\n",
    "\"\"\"\n",
    "## Course Complete! üéì\n",
    "\n",
    "### What You Built\n",
    "\n",
    "**Month 1: Data Analytics Foundations**\n",
    "- SQL querying for healthcare data\n",
    "- Python data analysis with pandas\n",
    "- Looker Studio dashboards\n",
    "- EDA and data cleaning\n",
    "\n",
    "**Month 2: Applied ML & MLOps**\n",
    "- No-show prediction model\n",
    "- Scikit-learn pipelines\n",
    "- FastAPI deployment\n",
    "- Docker containerization\n",
    "\n",
    "**Month 3: Generative AI & LLM Applications**\n",
    "\n",
    "*Week 8-9: Prompt Engineering*\n",
    "- LLM client wrapper\n",
    "- Prompt templates for healthcare\n",
    "- Few-shot and chain-of-thought patterns\n",
    "- Safety guardrails\n",
    "\n",
    "*Week 10: LangChain Integration*\n",
    "- LCEL chains\n",
    "- Custom tools (ML API integration)\n",
    "- Conversation memory\n",
    "- Healthcare agent\n",
    "\n",
    "*Week 11: RAG Pipeline*\n",
    "- Document loading and chunking\n",
    "- FAISS vector store\n",
    "- Retrieval chains\n",
    "- Conversational RAG\n",
    "\n",
    "*Week 12: Evaluation & Production*\n",
    "- Ragas evaluation\n",
    "- Hallucination detection\n",
    "- Safety red-teaming\n",
    "- Regression testing\n",
    "- Production monitoring\n",
    "\n",
    "### Your Healthcare Appointment Assistant Features\n",
    "\n",
    "1. ‚úÖ No-show risk prediction\n",
    "2. ‚úÖ Natural language explanations\n",
    "3. ‚úÖ Intervention recommendations\n",
    "4. ‚úÖ Policy Q&A (RAG)\n",
    "5. ‚úÖ Conversational interface\n",
    "6. ‚úÖ Safety guardrails\n",
    "7. ‚úÖ Production monitoring\n",
    "8. ‚úÖ Comprehensive evaluation\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "1. Deploy to cloud (AWS/GCP/Azure)\n",
    "2. Add more policy documents\n",
    "3. Integrate with EHR system\n",
    "4. Add user authentication\n",
    "5. Set up CI/CD for evaluations\n",
    "6. Monitor in production\n",
    "\"\"\"\n",
    "\n",
    "print(\"Congratulations on completing the course! üéâ\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
