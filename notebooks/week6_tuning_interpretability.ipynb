{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aec1134c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ SHAP is available\n",
      "‚úÖ All imports successful!\n",
      "üìÖ Analysis date: 2025-12-04 12:52:36\n",
      "üìã ML Configuration loaded!\n",
      "   Target: no_show\n",
      "   Primary metric: roc_auc\n",
      "   Tuning strategy: random\n",
      "   CV folds: 5\n",
      "\n",
      "üìä Data loaded: 110,527 rows √ó 53 columns\n",
      "\n",
      "üéØ Target distribution:\n",
      "no_show\n",
      "0    88208\n",
      "1    22319\n",
      "Name: count, dtype: int64\n",
      "\n",
      "   No-show rate: 20.2%\n",
      "\n",
      "üìä Data Split:\n",
      "   Training: 88,421 samples\n",
      "   Test: 22,106 samples\n",
      "   Features: 41\n",
      "\n",
      "üîÑ After Preprocessing:\n",
      "   Training shape: (88421, 57)\n",
      "   Test shape: (22106, 57)\n",
      "   Total features (after encoding): 57\n",
      "\n",
      "üìã Sample Feature Names (after encoding):\n",
      "    1. numeric__age\n",
      "    2. numeric__lead_days\n",
      "    3. numeric__patient_total_appointments\n",
      "    4. numeric__patient_previous_noshows\n",
      "    5. numeric__patient_previous_appointments\n",
      "    6. numeric__patient_historical_noshow_rate\n",
      "    7. numeric__days_since_last_appointment\n",
      "    8. numeric__days_since_last_noshow\n",
      "    9. numeric__total_conditions\n",
      "   10. numeric__neighborhood_noshow_rate\n",
      "   11. numeric__neighborhood_scholarship_rate\n",
      "   12. numeric__neighborhood_avg_age\n",
      "   13. categorical__gender_M\n",
      "   14. categorical__age_group_Elderly\n",
      "   15. categorical__age_group_Middle Age\n",
      "   16. categorical__age_group_Senior\n",
      "   17. categorical__age_group_Teen\n",
      "   18. categorical__age_group_Young Adult\n",
      "   19. categorical__lead_time_category_15-30 days\n",
      "   20. categorical__lead_time_category_3-7 days\n",
      "   ... and 37 more features\n",
      "üîß Created Pipelines:\n",
      "   ‚Ä¢ logistic_regression\n",
      "   ‚Ä¢ decision_tree\n",
      "   ‚Ä¢ random_forest\n",
      "   ‚Ä¢ gradient_boosting\n",
      "   ‚Ä¢ xgboost\n",
      "\n",
      "üìã Random Forest Pipeline Structure:\n",
      "----------------------------------------\n",
      "Step: preprocessor\n",
      "  Type: ColumnTransformer\n",
      "Step: classifier\n",
      "  Type: RandomForestClassifier\n",
      "‚öôÔ∏è Hyperparameter Tuner Configuration:\n",
      "   Strategy: random\n",
      "   Max iterations: 50\n",
      "   CV folds: 5\n",
      "   Scoring metric: roc_auc\n",
      "\n",
      "üìã Parameter Grids to Search:\n",
      "--------------------------------------------------\n",
      "\n",
      "logistic_regression:\n",
      "   C: [0.001, 0.01, 0.1, 1, 10]...\n",
      "   penalty: ['l1', 'l2']\n",
      "   solver: ['liblinear', 'saga']\n",
      "   class_weight: ['balanced', None]\n",
      "   max_iter: [1000]\n",
      "\n",
      "random_forest:\n",
      "   n_estimators: [50, 100, 200, 300]\n",
      "   max_depth: [5, 10, 15, 20, None]\n",
      "   min_samples_split: [2, 5, 10, 20]\n",
      "   min_samples_leaf: [1, 2, 4, 8]\n",
      "   max_features: ['sqrt', 'log2', 0.5]\n",
      "   class_weight: ['balanced', 'balanced_subsample']\n",
      "   bootstrap: [True, False]\n",
      "\n",
      "gradient_boosting:\n",
      "   n_estimators: [50, 100, 200]\n",
      "   learning_rate: [0.01, 0.05, 0.1, 0.2]\n",
      "   max_depth: [3, 5, 7, 10]\n",
      "   min_samples_split: [2, 5, 10]\n",
      "   min_samples_leaf: [1, 2, 4]\n",
      "   subsample: [0.8, 0.9, 1.0]\n",
      "   max_features: ['sqrt', 'log2']\n",
      "\n",
      "xgboost:\n",
      "   n_estimators: [50, 100, 200, 300]\n",
      "   learning_rate: [0.01, 0.05, 0.1, 0.2]\n",
      "   max_depth: [3, 5, 7, 10]\n",
      "   min_child_weight: [1, 3, 5, 7]\n",
      "   gamma: [0, 0.1, 0.2, 0.3]\n",
      "   subsample: [0.7, 0.8, 0.9, 1.0]\n",
      "   colsample_bytree: [0.7, 0.8, 0.9, 1.0]\n",
      "   reg_alpha: [0, 0.01, 0.1, 1]\n",
      "   reg_lambda: [0, 0.01, 0.1, 1]\n",
      "   scale_pos_weight: [1, 3, 5]\n",
      "üîÑ Starting hyperparameter tuning...\n",
      "   This may take several minutes...\n",
      "\n",
      "   Including XGBoost in tuning\n",
      "\n",
      "Fitting 5 folds for each of 48 candidates, totalling 240 fits\n",
      "Fitting 5 folds for each of 50 candidates, totalling 250 fits\n"
     ]
    }
   ],
   "source": [
    "# %% [markdown]\n",
    "\"\"\"\n",
    "# Week 6: Model Tuning & Interpretability\n",
    "\n",
    "## Learning Objectives\n",
    "- Build reproducible sklearn pipelines\n",
    "- Tune hyperparameters with GridSearchCV and RandomizedSearchCV\n",
    "- Interpret models using SHAP values\n",
    "- Understand feature importance from multiple perspectives\n",
    "- Optimize decision thresholds for business objectives\n",
    "\n",
    "## Prerequisites\n",
    "- Completed Week 5 (baseline models trained)\n",
    "- SHAP library installed (`pip install shap`)\n",
    "\n",
    "## What We'll Cover\n",
    "1. **sklearn Pipelines** - End-to-end reproducible workflows\n",
    "2. **Hyperparameter Tuning** - Finding optimal model parameters\n",
    "3. **SHAP Analysis** - Understanding model predictions\n",
    "4. **Feature Importance** - Comparing importance methods\n",
    "5. **Threshold Optimization** - Business-driven decision making\n",
    "\"\"\"\n",
    "\n",
    "# %% [markdown]\n",
    "\"\"\"\n",
    "## 1. Setup and Configuration\n",
    "\"\"\"\n",
    "\n",
    "# %%\n",
    "# Standard imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "import sys\n",
    "import json\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "# Add src to path\n",
    "sys.path.insert(0, str(Path.cwd().parent))\n",
    "\n",
    "# sklearn imports\n",
    "from sklearn.model_selection import (\n",
    "    train_test_split, cross_val_score, \n",
    "    GridSearchCV, RandomizedSearchCV,\n",
    "    StratifiedKFold, learning_curve\n",
    ")\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import (\n",
    "    roc_auc_score, f1_score, precision_score, recall_score,\n",
    "    confusion_matrix, classification_report, roc_curve,\n",
    "    precision_recall_curve\n",
    ")\n",
    "\n",
    "# Our ML modules\n",
    "from src.ml.preprocessing import NoShowPreprocessor\n",
    "from src.ml.train import ModelTrainer\n",
    "from src.ml.evaluate import ModelEvaluator\n",
    "from src.ml.pipeline import NoShowPipelineBuilder, PipelineManager\n",
    "from src.ml.tuning import HyperparameterTuner\n",
    "from src.ml.interpret import ModelInterpreter, create_interpretation_report\n",
    "\n",
    "import yaml\n",
    "import joblib\n",
    "\n",
    "# SHAP (optional but recommended)\n",
    "try:\n",
    "    import shap\n",
    "    SHAP_AVAILABLE = True\n",
    "    print(\"‚úÖ SHAP is available\")\n",
    "except ImportError:\n",
    "    SHAP_AVAILABLE = False\n",
    "    print(\"‚ö†Ô∏è SHAP not installed. Run: pip install shap\")\n",
    "\n",
    "# Settings\n",
    "warnings.filterwarnings('ignore')\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "pd.set_option('display.max_columns', 50)\n",
    "pd.set_option('display.float_format', '{:.4f}'.format)\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "RANDOM_STATE = 42\n",
    "np.random.seed(RANDOM_STATE)\n",
    "\n",
    "print(\"‚úÖ All imports successful!\")\n",
    "print(f\"üìÖ Analysis date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "\n",
    "# %%\n",
    "# Load configurations\n",
    "with open('../config/ml_config.yaml', 'r') as f:\n",
    "    ml_config = yaml.safe_load(f)\n",
    "\n",
    "print(\"üìã ML Configuration loaded!\")\n",
    "print(f\"   Target: {ml_config['ml_project']['target_column']}\")\n",
    "print(f\"   Primary metric: {ml_config['evaluation']['primary_metric']}\")\n",
    "print(f\"   Tuning strategy: {ml_config['tuning']['strategy']}\")\n",
    "print(f\"   CV folds: {ml_config['tuning']['cv_folds']}\")\n",
    "\n",
    "# %%\n",
    "# Load processed data\n",
    "data_path = '../data/processed/appointments_features.csv'\n",
    "df = pd.read_csv(data_path)\n",
    "\n",
    "print(f\"\\nüìä Data loaded: {df.shape[0]:,} rows √ó {df.shape[1]} columns\")\n",
    "print(f\"\\nüéØ Target distribution:\")\n",
    "print(df['no_show'].value_counts())\n",
    "print(f\"\\n   No-show rate: {df['no_show'].mean():.1%}\")\n",
    "\n",
    "# %% [markdown]\n",
    "\"\"\"\n",
    "## 2. Data Preparation with Preprocessing Pipeline\n",
    "\n",
    "We'll use our `NoShowPreprocessor` class to create a reproducible\n",
    "preprocessing pipeline that handles:\n",
    "- Feature selection\n",
    "- Missing value imputation\n",
    "- Categorical encoding\n",
    "- Numeric scaling\n",
    "\"\"\"\n",
    "\n",
    "# %%\n",
    "# Initialize preprocessor\n",
    "preprocessor = NoShowPreprocessor(ml_config)\n",
    "\n",
    "# Prepare data (select features and split)\n",
    "X_train, X_test, y_train, y_test = preprocessor.prepare_data(df)\n",
    "\n",
    "print(f\"\\nüìä Data Split:\")\n",
    "print(f\"   Training: {len(X_train):,} samples\")\n",
    "print(f\"   Test: {len(X_test):,} samples\")\n",
    "print(f\"   Features: {X_train.shape[1]}\")\n",
    "\n",
    "# %%\n",
    "# Fit and transform training data\n",
    "X_train_transformed = preprocessor.fit_transform(X_train, y_train)\n",
    "X_test_transformed = preprocessor.transform(X_test)\n",
    "\n",
    "print(f\"\\nüîÑ After Preprocessing:\")\n",
    "print(f\"   Training shape: {X_train_transformed.shape}\")\n",
    "print(f\"   Test shape: {X_test_transformed.shape}\")\n",
    "print(f\"   Total features (after encoding): {len(preprocessor.feature_names_)}\")\n",
    "\n",
    "# %%\n",
    "# Show sample of feature names\n",
    "print(\"\\nüìã Sample Feature Names (after encoding):\")\n",
    "for i, name in enumerate(preprocessor.feature_names_[:20]):\n",
    "    print(f\"   {i+1:2d}. {name}\")\n",
    "if len(preprocessor.feature_names_) > 20:\n",
    "    print(f\"   ... and {len(preprocessor.feature_names_) - 20} more features\")\n",
    "\n",
    "# %% [markdown]\n",
    "\"\"\"\n",
    "## 3. Building sklearn Pipelines\n",
    "\n",
    "sklearn Pipelines chain preprocessing and modeling steps together,\n",
    "ensuring:\n",
    "- No data leakage (preprocessing fit only on training data)\n",
    "- Reproducibility (same transformations applied consistently)\n",
    "- Easy deployment (single object to save/load)\n",
    "\"\"\"\n",
    "\n",
    "# %%\n",
    "# Create pipelines for different models using our PipelineBuilder\n",
    "pipeline_manager = PipelineManager(ml_config)\n",
    "\n",
    "# Create pipelines for all models\n",
    "pipelines = pipeline_manager.create_all_pipelines(X_train)\n",
    "\n",
    "print(\"üîß Created Pipelines:\")\n",
    "for name in pipelines.keys():\n",
    "    print(f\"   ‚Ä¢ {name}\")\n",
    "\n",
    "# %%\n",
    "# Examine one pipeline structure\n",
    "sample_pipeline = pipelines['random_forest']\n",
    "print(\"\\nüìã Random Forest Pipeline Structure:\")\n",
    "print(\"-\" * 40)\n",
    "for step_name, step in sample_pipeline.named_steps.items():\n",
    "    print(f\"Step: {step_name}\")\n",
    "    print(f\"  Type: {type(step).__name__}\")\n",
    "\n",
    "# %% [markdown]\n",
    "\"\"\"\n",
    "## 4. Hyperparameter Tuning\n",
    "\n",
    "We'll use RandomizedSearchCV to efficiently search the hyperparameter space.\n",
    "This is more efficient than GridSearchCV when the search space is large.\n",
    "\n",
    "### Why Tune Hyperparameters?\n",
    "- Default parameters are rarely optimal\n",
    "- Model performance can vary significantly with different parameters\n",
    "- Prevents overfitting through proper regularization\n",
    "\"\"\"\n",
    "\n",
    "# %%\n",
    "# Initialize the hyperparameter tuner\n",
    "tuner = HyperparameterTuner(ml_config)\n",
    "\n",
    "print(\"‚öôÔ∏è Hyperparameter Tuner Configuration:\")\n",
    "print(f\"   Strategy: {tuner.strategy}\")\n",
    "print(f\"   Max iterations: {tuner.n_iter}\")\n",
    "print(f\"   CV folds: {tuner.cv_folds}\")\n",
    "print(f\"   Scoring metric: {tuner.scoring}\")\n",
    "\n",
    "# %%\n",
    "# Show parameter grids\n",
    "print(\"\\nüìã Parameter Grids to Search:\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "for model_name, params in tuner.param_grids.items():\n",
    "    print(f\"\\n{model_name}:\")\n",
    "    for param, values in params.items():\n",
    "        if isinstance(values, list):\n",
    "            print(f\"   {param}: {values[:5]}{'...' if len(values) > 5 else ''}\")\n",
    "        else:\n",
    "            print(f\"   {param}: {values}\")\n",
    "\n",
    "# %% [markdown]\n",
    "\"\"\"\n",
    "### 4.1 Tune All Models\n",
    "\n",
    "This will take some time depending on:\n",
    "- Size of parameter grids\n",
    "- Number of CV folds\n",
    "- Training data size\n",
    "\"\"\"\n",
    "\n",
    "# %%\n",
    "# Tune all models (this may take a few minutes)\n",
    "print(\"üîÑ Starting hyperparameter tuning...\")\n",
    "print(\"   This may take several minutes...\\n\")\n",
    "\n",
    "# We'll tune 3 key models for time efficiency\n",
    "# You can add more models as needed\n",
    "models_to_tune = ['logistic_regression', 'random_forest', 'gradient_boosting']\n",
    "\n",
    "# Add XGBoost if available\n",
    "try:\n",
    "    from xgboost import XGBClassifier\n",
    "    models_to_tune.append('xgboost')\n",
    "    print(\"   Including XGBoost in tuning\\n\")\n",
    "except ImportError:\n",
    "    print(\"   XGBoost not available, skipping\\n\")\n",
    "\n",
    "# Run tuning\n",
    "tuning_results = tuner.tune_all_models(\n",
    "    X_train_transformed, \n",
    "    y_train,\n",
    "    model_names=models_to_tune\n",
    ")\n",
    "\n",
    "# %%\n",
    "# Display tuning comparison\n",
    "comparison = tuner.get_comparison_table()\n",
    "print(\"\\nüìä Tuning Results Comparison:\")\n",
    "print(\"=\" * 70)\n",
    "print(comparison.to_string(index=False))\n",
    "\n",
    "# %%\n",
    "# Display best parameters for each model\n",
    "params_table = tuner.get_best_params_table()\n",
    "print(\"\\nüéØ Best Parameters Found:\")\n",
    "print(\"=\" * 70)\n",
    "for _, row in params_table.iterrows():\n",
    "    print(f\"\\n{row['Model']}:\")\n",
    "    for col in params_table.columns[1:]:\n",
    "        if pd.notna(row[col]):\n",
    "            print(f\"   {col}: {row[col]}\")\n",
    "\n",
    "# %%\n",
    "# Get the best overall model\n",
    "best_model_name, best_model = tuner.get_best_model()\n",
    "print(f\"\\nüèÜ Best Model: {best_model_name}\")\n",
    "print(f\"   Best {tuner.scoring}: {tuner.results[best_model_name].best_score:.4f}\")\n",
    "\n",
    "# %% [markdown]\n",
    "\"\"\"\n",
    "### 4.2 Visualize Tuning Results\n",
    "\"\"\"\n",
    "\n",
    "# %%\n",
    "# Plot tuning comparison\n",
    "output_dir = Path('../outputs/figures/ml/tuning')\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "fig = tuner.plot_comparison(save=True, output_dir=output_dir)\n",
    "plt.show()\n",
    "\n",
    "# %%\n",
    "# Plot CV score distributions\n",
    "fig = tuner.plot_cv_scores_distribution(save=True, output_dir=output_dir)\n",
    "plt.show()\n",
    "\n",
    "# %%\n",
    "# Plot learning curve for best model\n",
    "fig = tuner.plot_learning_curve(\n",
    "    best_model_name, \n",
    "    X_train_transformed, \n",
    "    y_train,\n",
    "    save=True, \n",
    "    output_dir=output_dir\n",
    ")\n",
    "plt.show()\n",
    "\n",
    "# %% [markdown]\n",
    "\"\"\"\n",
    "### 4.3 Evaluate Tuned Model on Test Set\n",
    "\"\"\"\n",
    "\n",
    "# %%\n",
    "# Evaluate best model on test set\n",
    "evaluator = ModelEvaluator(ml_config)\n",
    "\n",
    "# Evaluate tuned models\n",
    "tuned_results = evaluator.evaluate_all(\n",
    "    tuner.best_models, \n",
    "    X_test_transformed, \n",
    "    y_test\n",
    ")\n",
    "\n",
    "# %%\n",
    "# Display test results\n",
    "test_comparison = evaluator.get_comparison_table()\n",
    "print(\"\\nüìä Test Set Performance (Tuned Models):\")\n",
    "print(\"=\" * 70)\n",
    "print(test_comparison.to_string(index=False))\n",
    "\n",
    "# %%\n",
    "# Compare with baseline (Week 5) if available\n",
    "baseline_model_path = Path('../models/baseline/random_forest.joblib')\n",
    "if baseline_model_path.exists():\n",
    "    baseline_model = joblib.load(baseline_model_path)\n",
    "    baseline_pred = baseline_model.predict_proba(X_test_transformed)[:, 1]\n",
    "    baseline_auc = roc_auc_score(y_test, baseline_pred)\n",
    "    \n",
    "    tuned_pred = best_model.predict_proba(X_test_transformed)[:, 1]\n",
    "    tuned_auc = roc_auc_score(y_test, tuned_pred)\n",
    "    \n",
    "    print(f\"\\nüìà Improvement from Tuning:\")\n",
    "    print(f\"   Baseline ROC-AUC: {baseline_auc:.4f}\")\n",
    "    print(f\"   Tuned ROC-AUC:    {tuned_auc:.4f}\")\n",
    "    print(f\"   Improvement:      {(tuned_auc - baseline_auc)*100:+.2f}%\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è Baseline model not found for comparison\")\n",
    "\n",
    "# %%\n",
    "# Plot ROC curves for tuned models\n",
    "fig = evaluator.plot_roc_curves(save=True, filename=\"roc_curves_tuned.png\")\n",
    "plt.show()\n",
    "\n",
    "# %%\n",
    "# Plot confusion matrices\n",
    "fig = evaluator.plot_confusion_matrices(save=True, filename=\"confusion_matrices_tuned.png\")\n",
    "plt.show()\n",
    "\n",
    "# %% [markdown]\n",
    "\"\"\"\n",
    "## 5. Model Interpretability with SHAP\n",
    "\n",
    "SHAP (SHapley Additive exPlanations) helps us understand:\n",
    "- **Global interpretability**: Which features are most important overall?\n",
    "- **Local interpretability**: Why did the model make this specific prediction?\n",
    "\n",
    "### Key Concepts:\n",
    "- **SHAP value**: The contribution of each feature to a prediction\n",
    "- **Base value**: The average model output (expected value)\n",
    "- **Positive SHAP**: Feature pushes prediction toward no-show\n",
    "- **Negative SHAP**: Feature pushes prediction toward showing up\n",
    "\"\"\"\n",
    "\n",
    "# %%\n",
    "if SHAP_AVAILABLE:\n",
    "    # Initialize interpreter with best model\n",
    "    interpreter = ModelInterpreter(ml_config)\n",
    "    \n",
    "    # Fit interpreter with training data as background\n",
    "    # Use a sample for efficiency\n",
    "    background_samples = min(500, len(X_train_transformed))\n",
    "    X_background = X_train_transformed[:background_samples]\n",
    "    \n",
    "    interpreter.fit(\n",
    "        best_model,\n",
    "        X_background,\n",
    "        feature_names=preprocessor.feature_names_,\n",
    "        model_name=best_model_name\n",
    "    )\n",
    "    \n",
    "    print(f\"‚úÖ Interpreter fitted for {best_model_name}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è SHAP not available. Skipping SHAP analysis.\")\n",
    "    interpreter = None\n",
    "\n",
    "# %% [markdown]\n",
    "\"\"\"\n",
    "### 5.1 SHAP Summary Plots\n",
    "\n",
    "The summary plot shows:\n",
    "- Each point is a sample\n",
    "- Y-axis: Features (sorted by importance)\n",
    "- X-axis: SHAP value (impact on prediction)\n",
    "- Color: Feature value (red=high, blue=low)\n",
    "\"\"\"\n",
    "\n",
    "# %%\n",
    "if interpreter:\n",
    "    # Compute SHAP values for test set (use sample for speed)\n",
    "    shap_sample_size = min(1000, len(X_test_transformed))\n",
    "    X_shap = X_test_transformed[:shap_sample_size]\n",
    "    \n",
    "    # SHAP beeswarm plot\n",
    "    fig = interpreter.plot_shap_summary(X_shap, plot_type='dot', max_display=15)\n",
    "    plt.show()\n",
    "\n",
    "# %%\n",
    "if interpreter:\n",
    "    # SHAP bar plot (mean |SHAP| values)\n",
    "    fig = interpreter.plot_shap_bar(X_shap, max_display=15)\n",
    "    plt.show()\n",
    "\n",
    "# %% [markdown]\n",
    "\"\"\"\n",
    "### 5.2 SHAP Waterfall Plot\n",
    "\n",
    "The waterfall plot explains a single prediction:\n",
    "- Shows how each feature contributes\n",
    "- Starts from base value (average prediction)\n",
    "- Each feature pushes the prediction up or down\n",
    "\"\"\"\n",
    "\n",
    "# %%\n",
    "if interpreter:\n",
    "    # Find a high-risk prediction to explain\n",
    "    y_proba = best_model.predict_proba(X_test_transformed)[:, 1]\n",
    "    high_risk_indices = np.where(y_proba > 0.5)[0]\n",
    "    \n",
    "    if len(high_risk_indices) > 0:\n",
    "        # Pick a random high-risk case\n",
    "        sample_idx = high_risk_indices[0]\n",
    "        print(f\"üìã Explaining prediction for sample {sample_idx}\")\n",
    "        print(f\"   Predicted probability: {y_proba[sample_idx]:.1%}\")\n",
    "        print(f\"   Actual outcome: {'No-Show' if y_test.iloc[sample_idx] == 1 else 'Showed Up'}\")\n",
    "        \n",
    "        # Create waterfall plot\n",
    "        fig = interpreter.plot_shap_waterfall(X_test_transformed, sample_idx=sample_idx)\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(\"No high-risk predictions found\")\n",
    "\n",
    "# %% [markdown]\n",
    "\"\"\"\n",
    "### 5.3 SHAP Dependence Plots\n",
    "\n",
    "Dependence plots show how a feature's value affects predictions:\n",
    "- X-axis: Feature value\n",
    "- Y-axis: SHAP value\n",
    "- Color: Interacting feature\n",
    "\"\"\"\n",
    "\n",
    "# %%\n",
    "if interpreter:\n",
    "    # Get top features\n",
    "    shap_importance = interpreter.get_shap_importance()\n",
    "    if shap_importance:\n",
    "        top_features_df = shap_importance.to_dataframe().head(4)\n",
    "        print(\"üìä Top 4 features for dependence plots:\")\n",
    "        print(top_features_df.to_string(index=False))\n",
    "        \n",
    "        # Create dependence plots for top features\n",
    "        for i, row in top_features_df.head(2).iterrows():\n",
    "            feature_name = row['feature']\n",
    "            print(f\"\\nüìà Dependence plot for: {feature_name}\")\n",
    "            fig = interpreter.plot_shap_dependence(X_shap, feature_name)\n",
    "            plt.show()\n",
    "\n",
    "# %% [markdown]\n",
    "\"\"\"\n",
    "### 5.4 Individual Prediction Explanation\n",
    "\n",
    "Let's explain specific predictions in human-readable format.\n",
    "\"\"\"\n",
    "\n",
    "# %%\n",
    "if interpreter:\n",
    "    # Explain a few predictions\n",
    "    sample_indices = [0, 10, 50]  # Low, medium, high risk examples\n",
    "    \n",
    "    for idx in sample_indices:\n",
    "        if idx < len(X_test_transformed):\n",
    "            explanation = interpreter.explain_prediction(X_test_transformed[idx])\n",
    "            \n",
    "            print(f\"\\n{'='*60}\")\n",
    "            print(f\"PREDICTION EXPLANATION - Sample {idx}\")\n",
    "            print(f\"{'='*60}\")\n",
    "            print(f\"Prediction: {explanation.probability:.1%} no-show probability\")\n",
    "            print(f\"Risk Tier: {explanation.risk_tier}\")\n",
    "            \n",
    "            print(f\"\\nüî∫ Top factors INCREASING no-show risk:\")\n",
    "            for feat, val, impact in explanation.top_positive_features[:3]:\n",
    "                print(f\"   ‚Ä¢ {feat}: {impact:+.4f}\")\n",
    "            \n",
    "            print(f\"\\nüîª Top factors DECREASING no-show risk:\")\n",
    "            for feat, val, impact in explanation.top_negative_features[:3]:\n",
    "                print(f\"   ‚Ä¢ {feat}: {impact:+.4f}\")\n",
    "\n",
    "# %%\n",
    "if interpreter:\n",
    "    # Visual explanation for one prediction\n",
    "    high_risk_idx = np.argmax(y_proba)\n",
    "    explanation = interpreter.explain_prediction(X_test_transformed[high_risk_idx])\n",
    "    \n",
    "    fig = interpreter.plot_prediction_explanation(explanation)\n",
    "    plt.show()\n",
    "\n",
    "# %% [markdown]\n",
    "\"\"\"\n",
    "## 6. Feature Importance Comparison\n",
    "\n",
    "Let's compare feature importance from different methods:\n",
    "1. **Model-based** (e.g., tree feature_importances_)\n",
    "2. **Permutation importance** (model-agnostic)\n",
    "3. **SHAP values** (game-theoretic)\n",
    "\"\"\"\n",
    "\n",
    "# %%\n",
    "if interpreter:\n",
    "    # Generate comparison plot\n",
    "    fig = interpreter.plot_feature_importance_comparison(\n",
    "        X_test_transformed, \n",
    "        y_test,\n",
    "        top_n=15\n",
    "    )\n",
    "    plt.show()\n",
    "\n",
    "# %%\n",
    "# Get detailed importance tables\n",
    "if interpreter:\n",
    "    print(\"üìä Feature Importance Summary\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Model-based importance\n",
    "    model_imp = interpreter.get_model_feature_importance()\n",
    "    if model_imp:\n",
    "        print(\"\\nüå≤ Model-based Importance (top 10):\")\n",
    "        print(model_imp.to_dataframe().head(10).to_string(index=False))\n",
    "    \n",
    "    # SHAP importance\n",
    "    shap_imp = interpreter.get_shap_importance()\n",
    "    if shap_imp:\n",
    "        print(\"\\nüìä SHAP-based Importance (top 10):\")\n",
    "        print(shap_imp.to_dataframe().head(10).to_string(index=False))\n",
    "\n",
    "# %% [markdown]\n",
    "\"\"\"\n",
    "## 7. Partial Dependence Plots (PDP)\n",
    "\n",
    "PDPs show the marginal effect of features on predictions,\n",
    "averaging out the effects of all other features.\n",
    "\"\"\"\n",
    "\n",
    "# %%\n",
    "if interpreter:\n",
    "    # Create partial dependence plots for top features\n",
    "    fig = interpreter.plot_partial_dependence(X_test_transformed, n_cols=3)\n",
    "    plt.show()\n",
    "\n",
    "# %% [markdown]\n",
    "\"\"\"\n",
    "## 8. Threshold Optimization\n",
    "\n",
    "The default classification threshold is 0.5, but this may not be optimal\n",
    "for our business case. We want to balance:\n",
    "- **Precision**: Of predicted no-shows, how many are correct?\n",
    "- **Recall**: Of actual no-shows, how many did we catch?\n",
    "\n",
    "### Business Context:\n",
    "- **False Negative** (missed no-show): Costs $150 (empty slot)\n",
    "- **False Positive** (unnecessary intervention): Costs $10 (extra reminder)\n",
    "\n",
    "This means we should favor recall over precision!\n",
    "\"\"\"\n",
    "\n",
    "# %%\n",
    "# Get predictions\n",
    "y_proba = best_model.predict_proba(X_test_transformed)[:, 1]\n",
    "\n",
    "# Calculate metrics at different thresholds\n",
    "thresholds = np.arange(0.1, 0.9, 0.05)\n",
    "metrics_by_threshold = []\n",
    "\n",
    "for thresh in thresholds:\n",
    "    y_pred = (y_proba >= thresh).astype(int)\n",
    "    \n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    tn, fp, fn, tp = cm.ravel()\n",
    "    \n",
    "    metrics_by_threshold.append({\n",
    "        'threshold': thresh,\n",
    "        'precision': precision_score(y_test, y_pred, zero_division=0),\n",
    "        'recall': recall_score(y_test, y_pred, zero_division=0),\n",
    "        'f1': f1_score(y_test, y_pred, zero_division=0),\n",
    "        'true_positives': tp,\n",
    "        'false_positives': fp,\n",
    "        'false_negatives': fn,\n",
    "        'true_negatives': tn\n",
    "    })\n",
    "\n",
    "threshold_df = pd.DataFrame(metrics_by_threshold)\n",
    "\n",
    "# %%\n",
    "# Plot precision-recall tradeoff\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Plot 1: Metrics vs Threshold\n",
    "ax1 = axes[0]\n",
    "ax1.plot(threshold_df['threshold'], threshold_df['precision'], 'b-', label='Precision', linewidth=2)\n",
    "ax1.plot(threshold_df['threshold'], threshold_df['recall'], 'r-', label='Recall', linewidth=2)\n",
    "ax1.plot(threshold_df['threshold'], threshold_df['f1'], 'g--', label='F1 Score', linewidth=2)\n",
    "ax1.axvline(x=0.5, color='gray', linestyle=':', label='Default (0.5)')\n",
    "ax1.set_xlabel('Threshold', fontsize=12)\n",
    "ax1.set_ylabel('Score', fontsize=12)\n",
    "ax1.set_title('Precision, Recall, F1 vs Threshold', fontsize=14)\n",
    "ax1.legend(loc='center right')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Business Cost Analysis\n",
    "ax2 = axes[1]\n",
    "cost_fn = 150  # Cost of false negative (missed no-show)\n",
    "cost_fp = 10   # Cost of false positive (unnecessary intervention)\n",
    "\n",
    "threshold_df['total_cost'] = (\n",
    "    threshold_df['false_negatives'] * cost_fn + \n",
    "    threshold_df['false_positives'] * cost_fp\n",
    ")\n",
    "\n",
    "ax2.plot(threshold_df['threshold'], threshold_df['total_cost'], 'purple', linewidth=2)\n",
    "optimal_idx = threshold_df['total_cost'].idxmin()\n",
    "optimal_threshold = threshold_df.loc[optimal_idx, 'threshold']\n",
    "ax2.axvline(x=optimal_threshold, color='green', linestyle='--', \n",
    "           label=f'Optimal ({optimal_threshold:.2f})')\n",
    "ax2.axvline(x=0.5, color='gray', linestyle=':', label='Default (0.5)')\n",
    "ax2.set_xlabel('Threshold', fontsize=12)\n",
    "ax2.set_ylabel('Total Cost ($)', fontsize=12)\n",
    "ax2.set_title('Business Cost vs Threshold', fontsize=14)\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../outputs/figures/ml/threshold_optimization.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nüéØ Optimal Threshold: {optimal_threshold:.2f}\")\n",
    "print(f\"   At this threshold:\")\n",
    "print(f\"   - Precision: {threshold_df.loc[optimal_idx, 'precision']:.3f}\")\n",
    "print(f\"   - Recall: {threshold_df.loc[optimal_idx, 'recall']:.3f}\")\n",
    "print(f\"   - F1 Score: {threshold_df.loc[optimal_idx, 'f1']:.3f}\")\n",
    "\n",
    "# %%\n",
    "# Compare default vs optimal threshold\n",
    "print(\"\\nüìä Threshold Comparison:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for thresh_val in [0.5, optimal_threshold]:\n",
    "    y_pred = (y_proba >= thresh_val).astype(int)\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    tn, fp, fn, tp = cm.ravel()\n",
    "    \n",
    "    total_cost = fn * cost_fn + fp * cost_fp\n",
    "    \n",
    "    print(f\"\\nThreshold = {thresh_val:.2f}:\")\n",
    "    print(f\"   True Positives (correct no-show predictions): {tp}\")\n",
    "    print(f\"   False Positives (unnecessary interventions): {fp}\")\n",
    "    print(f\"   False Negatives (missed no-shows): {fn}\")\n",
    "    print(f\"   True Negatives (correct show-up predictions): {tn}\")\n",
    "    print(f\"   Precision: {tp/(tp+fp) if (tp+fp) > 0 else 0:.3f}\")\n",
    "    print(f\"   Recall: {tp/(tp+fn) if (tp+fn) > 0 else 0:.3f}\")\n",
    "    print(f\"   Estimated Cost: ${total_cost:,}\")\n",
    "\n",
    "# %% [markdown]\n",
    "\"\"\"\n",
    "## 9. Save Results and Models\n",
    "\"\"\"\n",
    "\n",
    "# %%\n",
    "# Create output directories\n",
    "output_base = Path('../outputs/experiments')\n",
    "experiment_dir = output_base / f\"week6_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "experiment_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"üìÅ Saving results to: {experiment_dir}\")\n",
    "\n",
    "# %%\n",
    "# Save tuned models\n",
    "models_dir = experiment_dir / 'models'\n",
    "models_dir.mkdir(exist_ok=True)\n",
    "\n",
    "for name, model in tuner.best_models.items():\n",
    "    model_path = models_dir / f\"{name}_tuned.joblib\"\n",
    "    joblib.dump(model, model_path)\n",
    "    print(f\"   Saved: {model_path.name}\")\n",
    "\n",
    "# Save preprocessor\n",
    "preprocessor.save(models_dir / 'preprocessor.joblib')\n",
    "print(f\"   Saved: preprocessor.joblib\")\n",
    "\n",
    "# %%\n",
    "# Save tuning results\n",
    "tuner.save_results(experiment_dir / 'tuning', save_models=False)\n",
    "\n",
    "# %%\n",
    "# Save evaluation results\n",
    "evaluator.save_results(experiment_dir / 'evaluation')\n",
    "\n",
    "# %%\n",
    "# Generate interpretation report\n",
    "if interpreter:\n",
    "    report = create_interpretation_report(\n",
    "        interpreter,\n",
    "        X_test_transformed,\n",
    "        y_test,\n",
    "        experiment_dir / 'reports' / 'interpretation_report.txt'\n",
    "    )\n",
    "    print(\"\\nüìÑ Interpretation Report (excerpt):\")\n",
    "    print(\"-\" * 60)\n",
    "    print(\"\\n\".join(report.split(\"\\n\")[:30]))\n",
    "    print(\"...\")\n",
    "\n",
    "# %%\n",
    "# Save experiment summary\n",
    "summary = {\n",
    "    'experiment_id': experiment_dir.name,\n",
    "    'timestamp': datetime.now().isoformat(),\n",
    "    'data': {\n",
    "        'total_samples': len(df),\n",
    "        'training_samples': len(X_train),\n",
    "        'test_samples': len(X_test),\n",
    "        'features': len(preprocessor.feature_names_)\n",
    "    },\n",
    "    'best_model': {\n",
    "        'name': best_model_name,\n",
    "        'cv_score': float(tuner.results[best_model_name].best_score),\n",
    "        'test_roc_auc': float(tuned_results[best_model_name].roc_auc),\n",
    "        'test_f1': float(tuned_results[best_model_name].f1),\n",
    "        'best_params': tuner.results[best_model_name].best_params\n",
    "    },\n",
    "    'threshold_optimization': {\n",
    "        'optimal_threshold': float(optimal_threshold),\n",
    "        'default_threshold': 0.5,\n",
    "        'cost_savings': float(\n",
    "            threshold_df[threshold_df['threshold'] == 0.5]['total_cost'].values[0] -\n",
    "            threshold_df.loc[optimal_idx, 'total_cost']\n",
    "        )\n",
    "    }\n",
    "}\n",
    "\n",
    "with open(experiment_dir / 'experiment_summary.json', 'w') as f:\n",
    "    json.dump(summary, f, indent=2, default=str)\n",
    "\n",
    "print(f\"\\n‚úÖ Experiment summary saved!\")\n",
    "\n",
    "# %% [markdown]\n",
    "\"\"\"\n",
    "## 10. Summary and Key Findings\n",
    "\"\"\"\n",
    "\n",
    "# %%\n",
    "print(\"=\" * 60)\n",
    "print(\"WEEK 6 SUMMARY: TUNING & INTERPRETABILITY\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"\\nüèÜ Best Model: {best_model_name}\")\n",
    "print(f\"\\nüìä Performance Metrics (Test Set):\")\n",
    "best_result = tuned_results[best_model_name]\n",
    "print(f\"   ‚Ä¢ ROC-AUC:    {best_result.roc_auc:.4f}\")\n",
    "print(f\"   ‚Ä¢ Accuracy:   {best_result.accuracy:.4f}\")\n",
    "print(f\"   ‚Ä¢ Precision:  {best_result.precision:.4f}\")\n",
    "print(f\"   ‚Ä¢ Recall:     {best_result.recall:.4f}\")\n",
    "print(f\"   ‚Ä¢ F1 Score:   {best_result.f1:.4f}\")\n",
    "\n",
    "print(f\"\\nüéõÔ∏è Hyperparameter Tuning:\")\n",
    "print(f\"   ‚Ä¢ Strategy: {tuner.strategy}\")\n",
    "print(f\"   ‚Ä¢ Models tuned: {len(tuner.results)}\")\n",
    "print(f\"   ‚Ä¢ Best CV score: {tuner.results[best_model_name].best_score:.4f}\")\n",
    "\n",
    "print(f\"\\nüéØ Threshold Optimization:\")\n",
    "print(f\"   ‚Ä¢ Default threshold: 0.50\")\n",
    "print(f\"   ‚Ä¢ Optimal threshold: {optimal_threshold:.2f}\")\n",
    "print(f\"   ‚Ä¢ Estimated cost savings: ${summary['threshold_optimization']['cost_savings']:,.0f}\")\n",
    "\n",
    "if interpreter and shap_imp:\n",
    "    print(f\"\\nüìä Top 5 Most Important Features (SHAP):\")\n",
    "    for i, row in shap_imp.to_dataframe().head(5).iterrows():\n",
    "        print(f\"   {i+1}. {row['feature']}: {row['importance']:.4f}\")\n",
    "\n",
    "print(f\"\\nüìÅ Results saved to: {experiment_dir}\")\n",
    "\n",
    "# %% [markdown]\n",
    "\"\"\"\n",
    "## 11. Next Steps (Week 7 Preview)\n",
    "\n",
    "In Week 7, we'll focus on **Deployment**:\n",
    "\n",
    "1. **Model Packaging**\n",
    "   - Create production-ready pipeline\n",
    "   - Version control for models\n",
    "   \n",
    "2. **FastAPI Service**\n",
    "   - REST API for predictions\n",
    "   - Input validation with Pydantic\n",
    "   \n",
    "3. **Docker Containerization**\n",
    "   - Dockerfile for the API\n",
    "   - docker-compose for local testing\n",
    "   \n",
    "4. **Documentation**\n",
    "   - API documentation\n",
    "   - Usage examples\n",
    "   - Performance benchmarks\n",
    "\n",
    "### Prepare for Week 7:\n",
    "```bash\n",
    "# Install deployment dependencies\n",
    "pip install fastapi uvicorn pydantic python-multipart\n",
    "\n",
    "# Optional: Install Docker\n",
    "# https://docs.docker.com/get-docker/\n",
    "\"\"\"\n",
    "\n",
    "plt.close('all')\n",
    "\n",
    "print(\"\\n‚úÖ Week 6 notebook complete!\")\n",
    "print(\" Ready for Week 7: Model Deployment with FastAPI\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
