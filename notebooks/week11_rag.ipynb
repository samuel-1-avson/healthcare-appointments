{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3337cedc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# notebooks/week11_rag.ipynb\n",
    "\n",
    "\"\"\"\n",
    "# Week 11: RAG & Vector Databases\n",
    "## Healthcare Policy Q&A System\n",
    "\n",
    "### Learning Objectives\n",
    "1. Load and process documents for RAG\n",
    "2. Implement text chunking strategies\n",
    "3. Create and query vector stores\n",
    "4. Build complete RAG pipelines\n",
    "5. Evaluate RAG quality\n",
    "\n",
    "### Setup\n",
    "\"\"\"\n",
    "\n",
    "# Cell 1: Setup\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "project_root = Path.cwd().parent\n",
    "sys.path.insert(0, str(project_root))\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "print(\"OpenAI API Key:\", \"‚úÖ\" if os.getenv(\"OPENAI_API_KEY\") else \"‚ùå\")\n",
    "\n",
    "\n",
    "# Cell 2: Create Sample Documents\n",
    "\"\"\"\n",
    "## Part 1: Document Preparation\n",
    "\n",
    "First, let's ensure we have policy documents to work with.\n",
    "\"\"\"\n",
    "\n",
    "# Create documents directory\n",
    "docs_dir = project_root / \"data\" / \"documents\"\n",
    "docs_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Check for existing documents\n",
    "existing_docs = list(docs_dir.glob(\"*.md\"))\n",
    "print(f\"Found {len(existing_docs)} markdown documents\")\n",
    "\n",
    "for doc in existing_docs:\n",
    "    print(f\"  - {doc.name}\")\n",
    "\n",
    "\n",
    "# Cell 3: Load Documents\n",
    "\"\"\"\n",
    "## Part 2: Document Loading\n",
    "\"\"\"\n",
    "\n",
    "from src.llm.rag import DocumentLoader, load_policy_documents\n",
    "\n",
    "# Create loader\n",
    "loader = DocumentLoader(base_path=str(docs_dir))\n",
    "\n",
    "# Load all documents\n",
    "documents = loader.load_directory()\n",
    "\n",
    "print(f\"\\nLoaded {len(documents)} documents\")\n",
    "print(f\"Loading stats: {loader.get_stats()}\")\n",
    "\n",
    "# Preview first document\n",
    "if documents:\n",
    "    print(\"\\n--- First Document Preview ---\")\n",
    "    print(f\"Source: {documents[0].metadata.get('source', 'Unknown')}\")\n",
    "    print(f\"Content: {documents[0].page_content[:500]}...\")\n",
    "\n",
    "\n",
    "# Cell 4: Text Chunking\n",
    "\"\"\"\n",
    "## Part 3: Text Chunking\n",
    "\n",
    "Split documents into manageable chunks for embedding.\n",
    "\"\"\"\n",
    "\n",
    "from src.llm.rag import TextChunker, ChunkingStrategy, analyze_chunks\n",
    "\n",
    "# Create chunker\n",
    "chunker = TextChunker(\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=200,\n",
    "    strategy=ChunkingStrategy.RECURSIVE\n",
    ")\n",
    "\n",
    "# Chunk documents\n",
    "chunks = chunker.chunk_documents(documents)\n",
    "\n",
    "print(f\"Created {len(chunks)} chunks from {len(documents)} documents\")\n",
    "print(f\"\\nChunk Analysis:\")\n",
    "analysis = analyze_chunks(chunks)\n",
    "for key, value in analysis.items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "\n",
    "# Preview chunks\n",
    "print(\"\\n--- Sample Chunks ---\")\n",
    "for i, chunk in enumerate(chunks[:3]):\n",
    "    print(f\"\\nChunk {i+1}:\")\n",
    "    print(f\"  Size: {len(chunk.page_content)} chars\")\n",
    "    print(f\"  Source: {chunk.metadata.get('filename', 'Unknown')}\")\n",
    "    print(f\"  Section: {chunk.metadata.get('section', 'N/A')}\")\n",
    "    print(f\"  Content: {chunk.page_content[:150]}...\")\n",
    "\n",
    "\n",
    "# Cell 5: Embeddings\n",
    "\"\"\"\n",
    "## Part 4: Embeddings\n",
    "\n",
    "Convert text chunks to vector embeddings.\n",
    "\"\"\"\n",
    "\n",
    "from src.llm.rag import EmbeddingsManager\n",
    "\n",
    "# Create embeddings manager\n",
    "embeddings_manager = EmbeddingsManager(\n",
    "    provider=\"openai\",\n",
    "    model_name=\"text-embedding-3-small\",\n",
    "    use_cache=True\n",
    ")\n",
    "\n",
    "print(f\"Embeddings Model: {embeddings_manager.get_model_info()}\")\n",
    "\n",
    "# Test embedding\n",
    "sample_texts = [\n",
    "    \"What is the cancellation policy?\",\n",
    "    \"How do I reschedule an appointment?\",\n",
    "    \"What happens if I miss my appointment?\"\n",
    "]\n",
    "\n",
    "embeddings = embeddings_manager.embed_texts(sample_texts)\n",
    "\n",
    "print(f\"\\nGenerated {len(embeddings)} embeddings\")\n",
    "print(f\"Embedding dimension: {len(embeddings[0])}\")\n",
    "\n",
    "# Check similarity\n",
    "import numpy as np\n",
    "\n",
    "def cosine_similarity(a, b):\n",
    "    return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))\n",
    "\n",
    "print(\"\\nSimilarity between questions:\")\n",
    "for i in range(len(sample_texts)):\n",
    "    for j in range(i+1, len(sample_texts)):\n",
    "        sim = cosine_similarity(embeddings[i], embeddings[j])\n",
    "        print(f\"  '{sample_texts[i][:30]}...' vs '{sample_texts[j][:30]}...': {sim:.3f}\")\n",
    "\n",
    "\n",
    "# Cell 6: Vector Store\n",
    "\"\"\"\n",
    "## Part 5: Vector Store\n",
    "\n",
    "Create and query a FAISS vector store.\n",
    "\"\"\"\n",
    "\n",
    "from src.llm.rag import VectorStoreManager\n",
    "\n",
    "# Create vector store\n",
    "vector_store = VectorStoreManager(\n",
    "    store_type=\"faiss\",\n",
    "    embeddings_manager=embeddings_manager\n",
    ")\n",
    "\n",
    "# Index documents\n",
    "vector_store.create_from_documents(\n",
    "    documents,\n",
    "    chunk=True,\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=200\n",
    ")\n",
    "\n",
    "print(f\"Vector store created: {vector_store.get_stats()}\")\n",
    "\n",
    "# Save for later use\n",
    "vector_store.save(\"healthcare_policies\")\n",
    "print(\"Vector store saved!\")\n",
    "\n",
    "\n",
    "# Cell 7: Basic Search\n",
    "\"\"\"\n",
    "### Basic Similarity Search\n",
    "\"\"\"\n",
    "\n",
    "# Search\n",
    "query = \"What happens if a patient misses their appointment?\"\n",
    "results = vector_store.search(query, k=3)\n",
    "\n",
    "print(f\"Query: {query}\\n\")\n",
    "print(\"Top 3 Results:\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "for i, doc in enumerate(results):\n",
    "    print(f\"\\n{i+1}. {doc.metadata.get('filename', 'Unknown')}\")\n",
    "    print(f\"   Section: {doc.metadata.get('section', 'N/A')}\")\n",
    "    print(f\"   Content: {doc.page_content[:200]}...\")\n",
    "\n",
    "\n",
    "# Cell 8: Search with Scores\n",
    "\"\"\"\n",
    "### Search with Similarity Scores\n",
    "\"\"\"\n",
    "\n",
    "results_with_scores = vector_store.search_with_scores(query, k=5)\n",
    "\n",
    "print(f\"Query: {query}\\n\")\n",
    "print(\"Results with scores:\")\n",
    "for doc, score in results_with_scores:\n",
    "    print(f\"  Score: {score:.4f} - {doc.metadata.get('filename', 'Unknown')}\")\n",
    "\n",
    "\n",
    "# Cell 9: MMR Search\n",
    "\"\"\"\n",
    "### Maximum Marginal Relevance (MMR) Search\n",
    "\n",
    "MMR provides diverse results, not just the most similar.\n",
    "\"\"\"\n",
    "\n",
    "mmr_results = vector_store.mmr_search(\n",
    "    query,\n",
    "    k=4,\n",
    "    fetch_k=10,\n",
    "    lambda_mult=0.5  # 0 = max diversity, 1 = max relevance\n",
    ")\n",
    "\n",
    "print(f\"Query: {query}\\n\")\n",
    "print(\"MMR Results (diverse):\")\n",
    "for i, doc in enumerate(mmr_results):\n",
    "    print(f\"\\n{i+1}. {doc.metadata.get('section', 'Unknown section')}\")\n",
    "    print(f\"   {doc.page_content[:150]}...\")\n",
    "\n",
    "\n",
    "# Cell 10: RAG Chain\n",
    "\"\"\"\n",
    "## Part 6: RAG Chains\n",
    "\n",
    "Build complete question-answering pipelines.\n",
    "\"\"\"\n",
    "\n",
    "from src.llm.rag.chains import RAGChain, ConversationalRAGChain\n",
    "\n",
    "# Create RAG chain\n",
    "rag = RAGChain(\n",
    "    vector_store=vector_store,\n",
    "    temperature=0.2,\n",
    "    retriever_k=4\n",
    ")\n",
    "\n",
    "# Test questions\n",
    "test_questions = [\n",
    "    \"What is the no-show policy?\",\n",
    "    \"How many reminders do patients receive before their appointment?\",\n",
    "    \"What should staff do for high-risk patients?\",\n",
    "    \"Can no-show fees be waived?\"\n",
    "]\n",
    "\n",
    "print(\"RAG Chain Responses:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for question in test_questions:\n",
    "    result = rag.ask(question, return_sources=True)\n",
    "    \n",
    "    print(f\"\\nüìù Q: {question}\")\n",
    "    print(f\"\\nüí¨ A: {result['answer'][:400]}...\")\n",
    "    print(f\"\\nüìö Sources: {len(result.get('sources', []))} documents\")\n",
    "    print(\"-\" * 60)\n",
    "\n",
    "\n",
    "# Cell 11: Conversational RAG\n",
    "\"\"\"\n",
    "### Conversational RAG\n",
    "\n",
    "Maintains context across multiple questions.\n",
    "\"\"\"\n",
    "\n",
    "conv_rag = ConversationalRAGChain(\n",
    "    vector_store=vector_store,\n",
    "    max_history=5\n",
    ")\n",
    "\n",
    "# Create session\n",
    "session_id = conv_rag.create_session()\n",
    "print(f\"Session created: {session_id}\\n\")\n",
    "\n",
    "# Multi-turn conversation\n",
    "conversation = [\n",
    "    \"What is the cancellation policy?\",\n",
    "    \"What if I need to cancel same-day?\",\n",
    "    \"Are there any exceptions to these rules?\",\n",
    "    \"How do I appeal a no-show fee?\"\n",
    "]\n",
    "\n",
    "print(\"Conversational RAG:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for question in conversation:\n",
    "    result = conv_rag.ask(session_id, question)\n",
    "    \n",
    "    print(f\"\\nüë§ User: {question}\")\n",
    "    print(f\"\\nü§ñ Assistant: {result['answer'][:300]}...\")\n",
    "    \n",
    "    if result.get('standalone_question'):\n",
    "        print(f\"\\n   [Rewritten: {result['standalone_question']}]\")\n",
    "    \n",
    "    print(\"-\" * 40)\n",
    "\n",
    "# View history\n",
    "print(\"\\n\\nConversation History:\")\n",
    "history = conv_rag.get_history(session_id)\n",
    "for msg in history:\n",
    "    role = \"üë§\" if msg[\"role\"] == \"user\" else \"ü§ñ\"\n",
    "    print(f\"{role}: {msg['content'][:100]}...\")\n",
    "\n",
    "\n",
    "# Cell 12: Citation RAG\n",
    "\"\"\"\n",
    "### RAG with Citations\n",
    "\"\"\"\n",
    "\n",
    "from src.llm.rag.chains import CitationRAGChain\n",
    "\n",
    "citation_rag = CitationRAGChain(vector_store=vector_store)\n",
    "\n",
    "result = citation_rag.ask(\"What are the consequences of multiple no-shows?\")\n",
    "\n",
    "print(\"Citation RAG Response:\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nAnswer:\\n{result['answer']}\")\n",
    "print(f\"\\nCitations:\")\n",
    "for cite in result['citations']:\n",
    "    print(f\"  [{cite['number']}] {cite['filename']} - {cite['section']}\")\n",
    "\n",
    "\n",
    "# Cell 13: Advanced Retriever\n",
    "\"\"\"\n",
    "## Part 7: Advanced Retrieval\n",
    "\n",
    "Using query expansion and reranking.\n",
    "\"\"\"\n",
    "\n",
    "from src.llm.rag.retriever import PolicyRetriever, RetrievalConfig\n",
    "\n",
    "# Configure advanced retrieval\n",
    "config = RetrievalConfig(\n",
    "    top_k=4,\n",
    "    search_type=\"mmr\",\n",
    "    use_query_expansion=True,\n",
    "    expansion_count=2\n",
    ")\n",
    "\n",
    "advanced_retriever = PolicyRetriever(\n",
    "    vector_store=vector_store,\n",
    "    config=config\n",
    ")\n",
    "\n",
    "# Test\n",
    "query = \"transportation help for appointments\"\n",
    "results = advanced_retriever.search_with_context(query)\n",
    "\n",
    "print(f\"Query: {query}\")\n",
    "print(f\"\\nExpanded search found {len(results['documents'])} documents\")\n",
    "print(f\"\\nContext preview:\\n{results['context'][:500]}...\")\n",
    "\n",
    "\n",
    "# Cell 14: RAG Evaluation\n",
    "\"\"\"\n",
    "## Part 8: RAG Evaluation\n",
    "\"\"\"\n",
    "\n",
    "from src.llm.rag.evaluation import RAGEvaluator, create_healthcare_golden_set\n",
    "\n",
    "# Create evaluator\n",
    "evaluator = RAGEvaluator(\n",
    "    thresholds={\n",
    "        \"faithfulness\": 0.7,\n",
    "        \"answer_relevancy\": 0.7,\n",
    "        \"context_used\": 0.5\n",
    "    }\n",
    ")\n",
    "\n",
    "# Create golden set\n",
    "golden = create_healthcare_golden_set()\n",
    "print(f\"Golden set has {len(golden.questions)} questions\")\n",
    "\n",
    "# Add samples by running through RAG\n",
    "questions, ground_truths = golden.to_eval_format()\n",
    "\n",
    "# Limit for demo\n",
    "demo_questions = questions[:5]\n",
    "demo_truths = ground_truths[:5]\n",
    "\n",
    "print(\"\\nRunning evaluation on 5 questions...\")\n",
    "evaluator.add_samples_from_chain(rag, demo_questions, demo_truths)\n",
    "\n",
    "# Run evaluation\n",
    "results = evaluator.evaluate()\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"EVALUATION RESULTS\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nEvaluator: {results['evaluator']}\")\n",
    "print(f\"Samples: {results['sample_count']}\")\n",
    "print(f\"Passed: {results['passed_count']}\")\n",
    "print(f\"Pass Rate: {results['pass_rate']:.1%}\")\n",
    "\n",
    "print(\"\\nMetric Summary:\")\n",
    "for metric, values in results.get('summary', {}).items():\n",
    "    print(f\"  {metric}: mean={values['mean']:.3f}, range=[{values['min']:.3f}, {values['max']:.3f}]\")\n",
    "\n",
    "\n",
    "# Cell 15: Save Evaluation Results\n",
    "\"\"\"\n",
    "### Save Evaluation Results\n",
    "\"\"\"\n",
    "\n",
    "eval_dir = project_root / \"evals\" / \"rag_eval_results\"\n",
    "eval_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "from datetime import datetime\n",
    "eval_file = eval_dir / f\"eval_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json\"\n",
    "\n",
    "evaluator.save_results(str(eval_file))\n",
    "print(f\"Evaluation saved to: {eval_file}\")\n",
    "\n",
    "\n",
    "# Cell 16: Testing with API\n",
    "\"\"\"\n",
    "## Part 9: API Testing\n",
    "\n",
    "Test the RAG endpoints (requires API to be running).\n",
    "\"\"\"\n",
    "\n",
    "import httpx\n",
    "\n",
    "API_BASE = \"http://localhost:8000/api/v1\"\n",
    "\n",
    "async def test_rag_api():\n",
    "    async with httpx.AsyncClient() as client:\n",
    "        # Create index\n",
    "        print(\"Creating index...\")\n",
    "        response = await client.post(\n",
    "            f\"{API_BASE}/rag/index/create\",\n",
    "            params={\"documents_path\": \"data/documents\"}\n",
    "        )\n",
    "        print(f\"Index creation: {response.json()}\")\n",
    "        \n",
    "        # Ask question\n",
    "        print(\"\\nAsking question...\")\n",
    "        response = await client.post(\n",
    "            f\"{API_BASE}/rag/ask\",\n",
    "            json={\n",
    "                \"question\": \"What is the no-show policy?\",\n",
    "                \"include_sources\": True\n",
    "            }\n",
    "        )\n",
    "        print(f\"Answer: {response.json()['answer'][:200]}...\")\n",
    "        \n",
    "        # Search\n",
    "        print(\"\\nSearching...\")\n",
    "        response = await client.get(\n",
    "            f\"{API_BASE}/rag/search\",\n",
    "            params={\"query\": \"cancellation\", \"k\": 3}\n",
    "        )\n",
    "        print(f\"Found {response.json()['count']} results\")\n",
    "\n",
    "# Uncomment to run:\n",
    "# import asyncio\n",
    "# asyncio.run(test_rag_api())\n",
    "\n",
    "\n",
    "# Cell 17: Exercises\n",
    "\"\"\"\n",
    "## Exercises\n",
    "\n",
    "### Exercise 1: Chunking Comparison\n",
    "Compare different chunking strategies and their impact on retrieval.\n",
    "\"\"\"\n",
    "\n",
    "# Your code here:\n",
    "# chunking_strategies = [\n",
    "#     {\"strategy\": \"fixed\", \"size\": 500},\n",
    "#     {\"strategy\": \"recursive\", \"size\": 1000},\n",
    "#     {\"strategy\": \"markdown\", \"size\": 1000}\n",
    "# ]\n",
    "# \n",
    "# for config in chunking_strategies:\n",
    "#     # Create chunker with config\n",
    "#     # Count chunks\n",
    "#     # Evaluate retrieval quality\n",
    "#     pass\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "### Exercise 2: Custom Evaluation Set\n",
    "Create your own evaluation questions specific to your use case.\n",
    "\"\"\"\n",
    "\n",
    "# Your code here:\n",
    "# custom_golden = GoldenDataset(\"evals/custom_golden.json\")\n",
    "# \n",
    "# custom_golden.add_question(\n",
    "#     question=\"...\",\n",
    "#     expected_answer=\"...\",\n",
    "#     category=\"...\"\n",
    "# )\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "### Exercise 3: Hybrid Retrieval\n",
    "Implement a hybrid retriever that combines keyword and semantic search.\n",
    "\"\"\"\n",
    "\n",
    "# Your code here:\n",
    "# class HybridRetriever:\n",
    "#     def __init__(self, vector_store, keyword_weight=0.3):\n",
    "#         pass\n",
    "#     \n",
    "#     def search(self, query, k=4):\n",
    "#         # Combine keyword and semantic results\n",
    "#         pass\n",
    "\n",
    "\n",
    "# Cell 18: Summary\n",
    "\"\"\"\n",
    "## Summary\n",
    "\n",
    "This week you learned:\n",
    "\n",
    "1. **Document Loading**\n",
    "   - Load markdown, text, and other documents\n",
    "   - Extract metadata for better retrieval\n",
    "\n",
    "2. **Text Chunking**\n",
    "   - Recursive splitting respects document structure\n",
    "   - Overlap prevents information loss at boundaries\n",
    "   - Chunk size affects retrieval precision\n",
    "\n",
    "3. **Embeddings**\n",
    "   - Convert text to vectors for similarity search\n",
    "   - OpenAI and local embedding options\n",
    "   - Caching for efficiency\n",
    "\n",
    "4. **Vector Stores**\n",
    "   - FAISS for fast local similarity search\n",
    "   - Persistence for reloading indices\n",
    "   - MMR for diverse results\n",
    "\n",
    "5. **RAG Chains**\n",
    "   - Basic Q&A with retrieval\n",
    "   - Conversational RAG with history\n",
    "   - Citation-aware responses\n",
    "\n",
    "6. **Evaluation**\n",
    "   - Ragas metrics for quality assessment\n",
    "   - Golden datasets for regression testing\n",
    "   - Custom evaluation thresholds\n",
    "\n",
    "## Deliverables\n",
    "\n",
    "1. ‚úÖ Working document loader\n",
    "2. ‚úÖ Chunking pipeline\n",
    "3. ‚úÖ Vector store with FAISS\n",
    "4. ‚úÖ RAG chain for Q&A\n",
    "5. ‚úÖ Conversational RAG\n",
    "6. ‚úÖ Evaluation framework\n",
    "7. üìù Complete exercises\n",
    "8. üìù Custom golden set\n",
    "\"\"\"\n",
    "\n",
    "print(\"Week 11 Complete! üéâ\")\n",
    "\n",
    "\n",
    "# Cell 19: Stats\n",
    "\"\"\"\n",
    "### Final Statistics\n",
    "\"\"\"\n",
    "\n",
    "print(\"RAG Pipeline Stats:\")\n",
    "print(\"-\" * 40)\n",
    "print(f\"Documents loaded: {len(documents)}\")\n",
    "print(f\"Chunks created: {len(chunks)}\")\n",
    "print(f\"Vector store: {vector_store.get_stats()}\")\n",
    "print(f\"Embeddings: {embeddings_manager.get_stats()}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
