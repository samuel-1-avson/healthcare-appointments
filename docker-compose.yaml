# docker-compose.yaml
# ============================================================
# Healthcare No-Show Prediction System - Docker Compose
# ============================================================
# 
# Usage:
#   Start:          docker-compose up -d
#   Stop:           docker-compose down
#   Logs:           docker-compose logs -f api
#   Rebuild:        docker-compose build --no-cache
#   Shell:          docker-compose exec api bash
#
# ============================================================

services:
  # ==================== Frontend Service ====================
  frontend:
    build:
      context: ./frontend
      dockerfile: Dockerfile
    image: healthcare-noshow-frontend:latest
    container_name: noshow-frontend
    ports:
      - "3000:80"
    restart: unless-stopped
    networks:
      - noshow-network
    depends_on:
      api:
        condition: service_healthy

  # ==================== Main API Service ====================
  api:
    build:
      context: .
      dockerfile: Dockerfile
      target: production
    image: healthcare-noshow-api:latest
    # container_name: noshow-api  <-- Removed to allow scaling (conflicts with replicas)
    
    # Port mapping - REMOVED for load balancing (exposed via Nginx)
    # ports:
    #   - "${API_PORT:-8000}:8000"
    
    # Environment variables
    environment:
      # API Settings
      - NOSHOW_HOST=0.0.0.0
      - NOSHOW_PORT=8000
      - NOSHOW_DEBUG=${DEBUG:-false}
      - NOSHOW_LOG_LEVEL=${LOG_LEVEL:-INFO}
      - NOSHOW_WORKERS=${WORKERS:-2}
      
      # Model paths
      - NOSHOW_MODEL_PATH=/app/models/production/model.joblib
      - NOSHOW_PREPROCESSOR_PATH=/app/models/production/preprocessor.joblib
      - NOSHOW_METADATA_PATH=/app/models/production/model_metadata.json
      
      # LLM Settings
      - OPENAI_API_KEY=${OPENAI_API_KEY}
      - ANTHROPIC_API_KEY=${ANTHROPIC_API_KEY:-}
      - NOSHOW_LLM_DEFAULT_PROVIDER=${LLM_PROVIDER:-local}
      - NOSHOW_LLM_DEFAULT_MODEL=${LLM_MODEL:-llama3}
      - NOSHOW_LLM_CACHE_ENABLED=${LLM_CACHE:-true}
      # Ollama URL (use host.docker.internal to reach host machine from container)
      - OLLAMA_BASE_URL=${OLLAMA_BASE_URL:-http://host.docker.internal:11434}
      
      # LangChain/LangSmith (optional)
      - LANGCHAIN_TRACING_V2=${LANGCHAIN_TRACING:-false}
      - LANGCHAIN_API_KEY=${LANGCHAIN_API_KEY:-}
      - LANGCHAIN_PROJECT=${LANGCHAIN_PROJECT:-healthcare-assistant}
      
      # Database
      - NOSHOW_DATABASE_URL=postgresql://${POSTGRES_USER:-admin}:${POSTGRES_PASSWORD:-admin123}@postgres:5432/${POSTGRES_DB:-healthcare}
      
      # Celery
      - CELERY_BROKER_URL=amqp://guest:guest@rabbitmq:5672//
      - CELERY_RESULT_BACKEND=redis://redis:6379/0
    
    # Volume mounts
    volumes:
      # Models (read-only in production)
      - ./models/production:/app/models/production:ro
      # Configuration
      - ./config:/app/config:ro
      # Data directories for dashboard and predictions
      - ./data/raw:/app/data/raw:ro
      - ./data/processed:/app/data/processed:ro
      - ./data/dashboard:/app/data/dashboard:ro
      # Policy documents for RAG
      - ./data/documents:/app/data/documents:ro
      # Vector store (read-write for caching)
      - vector-store:/app/data/vector_store
      # Embeddings cache
      - embeddings-cache:/app/data/embeddings_cache
      # Logs
      - ./logs:/app/logs
      # Evaluation results
      - ./evals:/app/evals
      # Outputs (for patient alerts data)
      - ./outputs:/app/outputs:ro
    
    # Resource limits & Scaling
    deploy:
      replicas: 3
      resources:
        limits:
          cpus: '4'
          memory: 4G
        reservations:
          cpus: '1'
          memory: 1G
    
    # Health check
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s
    
    # Restart policy
    restart: unless-stopped
    
    # Logging configuration
    logging:
      driver: "json-file"
      options:
        max-size: "50m"
        max-file: "5"
    
    # Network
    networks:
      - noshow-network
    
    depends_on:
      postgres:
        condition: service_healthy
      redis:
        condition: service_healthy
      rabbitmq:
        condition: service_healthy

  # ==================== Nginx Load Balancer ====================
  nginx:
    image: nginx:alpine
    container_name: noshow-nginx
    ports:
      - "${NGINX_PORT:-8080}:80"
    volumes:
      - ./nginx/nginx.conf:/etc/nginx/nginx.conf:ro
    depends_on:
      api:
        condition: service_healthy
    networks:
      - noshow-network
    restart: unless-stopped

  # ==================== PostgreSQL Database ====================
  postgres:
    image: postgres:15-alpine
    container_name: noshow-postgres
    environment:
      - POSTGRES_USER=${POSTGRES_USER:-admin}
      - POSTGRES_PASSWORD=${POSTGRES_PASSWORD:-admin123}
      - POSTGRES_DB=${POSTGRES_DB:-healthcare}
    ports:
      - "${POSTGRES_PORT:-5432}:5432"
    volumes:
      - postgres-data:/var/lib/postgresql/data
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U admin -d healthcare"]
      interval: 10s
      timeout: 5s
      retries: 5
    networks:
      - noshow-network
    restart: unless-stopped

  # ==================== Redis Cache ====================
  redis:
    image: redis:7-alpine
    container_name: noshow-redis
    ports:
      - "${REDIS_PORT:-6379}:6379"
    volumes:
      - redis-data:/data
    command: redis-server --appendonly yes --maxmemory 256mb --maxmemory-policy allkeys-lru
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 10s
      timeout: 5s
      retries: 3
    networks:
      - noshow-network
    restart: unless-stopped

  # ==================== Observability ====================
  prometheus:
    image: prom/prometheus:latest
    container_name: noshow-prometheus
    volumes:
      - ./config/prometheus.yml:/etc/prometheus/prometheus.yml
    ports:
      - "9090:9090"
    networks:
      - noshow-network
    depends_on:
      - api

  grafana:
    image: grafana/grafana:latest
    container_name: noshow-grafana
    ports:
      - "3001:3000"
    volumes:
      - grafana-data:/var/lib/grafana
    networks:
      - noshow-network
    depends_on:
      - prometheus

  # ==================== Message Broker ====================
  rabbitmq:
    image: rabbitmq:3-management-alpine
    container_name: noshow-rabbitmq
    ports:
      - "5672:5672"
      - "15672:15672"
    volumes:
      - rabbitmq-data:/var/lib/rabbitmq
    healthcheck:
      test: ["CMD", "rabbitmq-diagnostics", "-q", "ping"]
      interval: 30s
      timeout: 10s
      retries: 3
    networks:
      - noshow-network
    restart: unless-stopped

  # ==================== Celery Worker ====================
  worker:
    build:
      context: .
      dockerfile: Dockerfile
      target: production
    image: healthcare-noshow-worker:latest
    container_name: noshow-worker
    command: celery -A src.api.worker.celery_app worker --loglevel=info
    environment:
      - NOSHOW_HOST=0.0.0.0
      - NOSHOW_PORT=8000
      - NOSHOW_DEBUG=${DEBUG:-false}
      - NOSHOW_LOG_LEVEL=${LOG_LEVEL:-INFO}
      - NOSHOW_MODEL_PATH=/app/models/production/model.joblib
      - NOSHOW_PREPROCESSOR_PATH=/app/models/production/preprocessor.joblib
      - NOSHOW_METADATA_PATH=/app/models/production/model_metadata.json
      - NOSHOW_DATABASE_URL=postgresql://${POSTGRES_USER:-admin}:${POSTGRES_PASSWORD:-admin123}@postgres:5432/${POSTGRES_DB:-healthcare}
      - CELERY_BROKER_URL=amqp://guest:guest@rabbitmq:5672//
      - CELERY_RESULT_BACKEND=redis://redis:6379/0
    volumes:
      - ./models/production:/app/models/production:ro
      - ./config:/app/config:ro
      - ./logs:/app/logs
    depends_on:
      rabbitmq:
        condition: service_healthy
      redis:
        condition: service_healthy
      postgres:
        condition: service_healthy
    networks:
      - noshow-network
    restart: unless-stopped

# ==================== MLflow Model Registry ====================
  mlflow:
    image: ghcr.io/mlflow/mlflow:v2.10.0
    container_name: noshow-mlflow
    ports:
      - "5000:5000"
    command: mlflow server --backend-store-uri sqlite:///mlflow.db --default-artifact-root /mlartifacts --host 0.0.0.0
    volumes:
      - ./mlflow:/mlflow
      - ./mlartifacts:/mlartifacts
    networks:
      - noshow-network
    restart: unless-stopped

# ==================== Networks ====================
networks:
  noshow-network:
    driver: bridge
    name: noshow-network

# ==================== Volumes ====================
volumes:
  vector-store:
    name: noshow-vector-store
  embeddings-cache:
    name: noshow-embeddings-cache
  redis-data:
    name: noshow-redis-data
  postgres-data:
    name: noshow-postgres-data
  grafana-data:
    name: noshow-grafana-data
  rabbitmq-data:
    name: noshow-rabbitmq-data