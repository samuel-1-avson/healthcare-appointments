# Machine Learning Configuration
# ==============================
# Configuration for ML model training and evaluation

# ML Project Settings
ml_project:
  name: "No-Show Prediction Model"
  version: "1.0.0"
  task_type: "binary_classification"
  target_column: "no_show"
  positive_class: 1  # 1 = no-show (what we want to predict)

# Data Splitting
splitting:
  test_size: 0.2
  validation_size: 0.2  # Of training set
  stratify: true
  random_state: 42

# Cross-Validation
cross_validation:
  n_folds: 5
  shuffle: true
  scoring:
    - "roc_auc"
    - "f1"
    - "precision"
    - "recall"
    - "accuracy"

# Feature Configuration
features:
  # Numeric features (will be scaled)
  numeric:
    - "age"
    - "lead_days"
    - "patient_total_appointments"
    - "patient_previous_noshows"
    - "patient_historical_noshow_rate"
    - "neighborhood_noshow_rate"
    - "neighborhood_avg_age"
    - "schedule_hour"
  
  # Categorical features (will be encoded)
  categorical:
    - "gender"
    - "age_group"
    - "lead_time_category"
    - "appointment_weekday"
    - "neighborhood_risk"
    - "health_risk_category"
  
  # Binary features (pass-through)
  binary:
    - "scholarship"
    - "hypertension"
    - "diabetes"
    - "alcoholism"
    - "sms_received"
    - "is_first_appointment"
    - "is_monday"
    - "is_friday"
    - "is_weekend"
    - "has_chronic_condition"
    - "has_disability"
    - "young_long_lead"
    - "monday_long_lead"
    - "first_young"
    - "high_risk_no_sms"
    - "elderly_chronic"
  
  # Features to exclude from training
  exclude:
    - "patientid"
    - "appointmentid"
    - "scheduledday"
    - "appointmentday"
    - "neighbourhood"
    - "showed_up"
    - "composite_risk_score"
    - "risk_tier"
    - "risk_tier_display"
    - "risk_percentile"

# Preprocessing
preprocessing:
  # Numeric transformer
  numeric_strategy:
    imputer: "median"
    scaler: "standard"  # standard, minmax, robust
  
  # Categorical transformer  
  categorical_strategy:
    imputer: "most_frequent"
    encoder: "onehot"  # onehot, ordinal, target
    handle_unknown: "ignore"
  
  # Binary transformer
  binary_strategy:
    imputer: "most_frequent"

# Baseline Models Configuration
baseline_models:
  logistic_regression:
    enabled: true
    params:
      random_state: 42
      max_iter: 1000
      class_weight: "balanced"
      solver: "lbfgs"
      C: 1.0
  
  random_forest:
    enabled: true
    params:
      n_estimators: 100
      max_depth: 10
      min_samples_split: 5
      min_samples_leaf: 2
      random_state: 42
      class_weight: "balanced"
      n_jobs: -1
  
  gradient_boosting:
    enabled: true
    params:
      n_estimators: 100
      learning_rate: 0.1
      max_depth: 5
      min_samples_split: 5
      random_state: 42
  
  xgboost:
    enabled: true
    params:
      n_estimators: 100
      learning_rate: 0.1
      max_depth: 5
      random_state: 42
      scale_pos_weight: 4  # Handle class imbalance (~20% no-show)
      use_label_encoder: false
      eval_metric: "logloss"

# Evaluation Settings
evaluation:
  # Primary metric for model selection
  primary_metric: "roc_auc"
  
  # Threshold optimization
  threshold_optimization: true
  threshold_metric: "f1"  # Optimize threshold for F1
  
  # Business metrics
  business:
    cost_per_noshow: 150
    cost_per_false_positive: 10  # Cost of unnecessary intervention
    cost_per_false_negative: 150  # Cost of missed no-show
  
  # Plots to generate
  plots:
    - "confusion_matrix"
    - "roc_curve"
    - "precision_recall_curve"
    - "feature_importance"
    - "calibration_curve"
    - "learning_curve"

# Output Paths
output:
  models_dir: "models"
  baseline_dir: "models/baseline"
  experiments_dir: "outputs/experiments"
  figures_dir: "outputs/figures/ml"

# Experiment Tracking
experiment:
  track_experiments: true
  log_params: true
  log_metrics: true
  save_artifacts: true

# ==============================================
# ADD TO EXISTING ml_config.yaml
# ==============================================

# Hyperparameter Tuning Configuration
tuning:
  # Search strategy: 'grid', 'random', 'bayesian'
  strategy: "random"
  
  # Number of iterations for random search
  n_iter: 50
  
  # Cross-validation folds for tuning
  cv_folds: 5
  
  # Scoring metric for optimization
  scoring: "roc_auc"
  
  # Number of parallel jobs (-1 = all cores)
  n_jobs: -1
  
  # Verbosity level
  verbose: 1
  
  # Random state for reproducibility
  random_state: 42
  
  # Early stopping (for iterative models)
  early_stopping: true
  early_stopping_rounds: 10
  
  # Parameter grids for each model
  param_grids:
    logistic_regression:
      C: [0.001, 0.01, 0.1, 1, 10, 100]
      penalty: ["l1", "l2"]
      solver: ["liblinear", "saga"]
      class_weight: ["balanced", null]
      max_iter: [1000]
    
    random_forest:
      n_estimators: [50, 100, 200, 300]
      max_depth: [5, 10, 15, 20, null]
      min_samples_split: [2, 5, 10, 20]
      min_samples_leaf: [1, 2, 4, 8]
      max_features: ["sqrt", "log2", 0.5]
      class_weight: ["balanced", "balanced_subsample"]
      bootstrap: [true, false]
    
    gradient_boosting:
      n_estimators: [50, 100, 200]
      learning_rate: [0.01, 0.05, 0.1, 0.2]
      max_depth: [3, 5, 7, 10]
      min_samples_split: [2, 5, 10]
      min_samples_leaf: [1, 2, 4]
      subsample: [0.8, 0.9, 1.0]
      max_features: ["sqrt", "log2"]
    
    xgboost:
      n_estimators: [50, 100, 200, 300]
      learning_rate: [0.01, 0.05, 0.1, 0.2]
      max_depth: [3, 5, 7, 10]
      min_child_weight: [1, 3, 5, 7]
      gamma: [0, 0.1, 0.2, 0.3]
      subsample: [0.7, 0.8, 0.9, 1.0]
      colsample_bytree: [0.7, 0.8, 0.9, 1.0]
      reg_alpha: [0, 0.01, 0.1, 1]
      reg_lambda: [0, 0.01, 0.1, 1]
      scale_pos_weight: [1, 3, 5]

# Model Interpretability Configuration
interpretability:
  # SHAP settings
  shap:
    enabled: true
    # Number of samples for SHAP computation (null = all)
    max_samples: 1000
    # Background samples for KernelSHAP
    background_samples: 100
    # Plot settings
    plots:
      - "summary"
      - "bar"
      - "beeswarm"
      - "waterfall"
      - "dependence"
    # Number of top features for dependence plots
    top_features_dependence: 5
  
  # Permutation importance settings
  permutation:
    enabled: true
    n_repeats: 10
    random_state: 42
    scoring: "roc_auc"
  
  # Partial dependence plots
  pdp:
    enabled: true
    features: null  # null = auto-select top features
    n_features: 6
    grid_resolution: 50

# Pipeline Configuration
pipeline:
  # Whether to include preprocessing in the final pipeline
  include_preprocessing: true
  
  # Memory caching for pipeline
  memory: null  # Set to path for caching, e.g., "cache/"
  
  # Verbose pipeline
  verbose: false

# Experiment Tracking
experiment:
  # Track all experiments
  track_experiments: true
  
  # Log parameters and metrics
  log_params: true
  log_metrics: true
  
  # Save artifacts (models, plots, etc.)
  save_artifacts: true
  
  # Experiments directory
  experiments_dir: "outputs/experiments"
  
  # Naming convention: 'timestamp', 'uuid', 'sequential'
  naming: "timestamp"

# Output Paths (update if needed)
output:
  models_dir: "models"
  baseline_dir: "models/baseline"
  tuned_dir: "models/tuned"
  production_dir: "models/production"
  experiments_dir: "outputs/experiments"
  figures_dir: "outputs/figures/ml"
  reports_dir: "outputs/reports"